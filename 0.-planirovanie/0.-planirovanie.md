Этой статьёй я начну серию о том, как __мне__ видится автоматизация.
Мы попробуем взять средних размеров дата-центр link_my_dc_up и проработать всю схему автоматизации.

В link_my_dc_up 4 ДЦ, около 250 коммутаторов, полдюжины маршрутизаторов и пара файрволов.
Не фейсбук, но достаточно для того, чтобы глубоко задуматься об автоматизации. 
Бытует, впрочем, мнение, что если у вас больше 1 устройства, уже нужна автоматизация.
На самом деле тяжело представить, что кто-то сейчас может жить без неё или хотя бы пачки наколеночных скриптов.
Хотя я слышал, что есть такие конторы, где учёт IP-адресов ведётся в экселе, а каждое из тысяч сетевых устройств настраивается вручную и имеет свою неповторимую конфигурацию. Это, конечно, можно выдать за модерн-арт, но чувства инженера точно были оскорблены.
Итак, наша задача в этой серии выстроить систему, которая будет поддерживать актуальную конфигурацию на каждом устройстве в соответствии с его ролью и местоположением.
Мы минимизируем вплоть до нуля хождение в CLI руками - любые изменения в настройках устройств или дизайне сети должны быть формализованы и документированы - и только потом выкатываться на нужные элементы сети.

То есть например, если мы решили, что с этого момента стоечные коммутаторы в Казани!!! должны анонсировать две сети вместе одной, мы
    1) Сначала документируем изменения в системах
    2) Готовим целевую модель конфигурации
    3) Запускаем программу обновления конфигурации сети, которая вычисляет, что нужно удалить на каждом узле, что добавить, и приводит узлы к нужному состоянию.
Это вместо того, чтобы на каждое устройство идти руками, пусть даже с подготовленной копипастой.


В описываемых тут идеях я буду не оригинален. У Дмитрия Фиголя!!! есть отличный канал со стримами на эту тему.
Статьи во многих аспектах будут с ними пересекаться.

И как и в случае с СДСМ начнём мы с очень общих представлений и подхода к планированию. 
В качестве основы я выбрал один из современных дизайнов датацентровой сети - L3 Clos Fabric  с BGP в качестве протокола маршрутизации.
Строить сеть мы будем на этот раз на Juniper. 
Подробнее о сетевом дизайне в следующей статье!!!


Итак, наша цель - поддержка состояния, а если быть точнее, то обеспечение жизненного цикла конфигурации:
Иметь список всех устройств в сети, их расположения, ролей, моделей, версий ПО.
Создавать начальный конфиг.
Обновлять его до актуальной версии при изменениях.
Откатывать конфиг до предыдущей (или произвольной) версии.
Периодически проверять все устройства на предмет отхождения от актуального (ночью кто-то тихонько добавил правило в ACL) и приводить к актуальному.
Делать бэкапы конфигурации.

Звучит достаточно сложно для того, чтобы начать декомпозировать проект на вехи.

И будет их пять:

Инвентарная система, 
система управления IP-пространством, 
Система описания сетевых сервисов
Вендор-агностик конфигурационная модель
Вендор-специфик конфигурация.



Веха 1. Инвентарная система. 
Очевидно, мы хотим знать, какое оборудование, где стоит, к чему подключено.
Инвентарная система - неотъемлемая часть любого предприятия. 
Чаще всего для сетевых устройств предприятие имеет отдельную инвентарную систему, которая решает более специфичные задачи.
Это то, что называется DCIM - Data Center Inventory Management.

Для наших задач в ней мы будем хранить следующую информацию про устройство:
    - Инвентарный номер
    - Название/описание 
    - Тип (коммутатор, маршрутизатор, сервер итд)
    - Модель (Huawei CE12800, Juniper QFX5200 итд)
    - Характерные параметры (платы, интерфейсы итд)
    - Роль (Leaf, Spine, Border Router итд)
    - Локацию (регион, город, дата-центр, стойка, юнит)
    - Интерконнекты между устройствами
    - Топологию сети

Прекрасно понятно, что нам самим хочется знать всё это. 
Но поможет ли это в целях автоматизации?
Безусловно.
Например, мы знаем, что в данном дата-центре на Leaf-коммутаторах, если это Huawei, на VLAN должны применяться ACL для фильтрации определённого трафика, а если это Juniper, то на физический интерфейс.
Или нужно раскатить новый SYSlog-сервер на все бордеры региона.

В ней же мы будем хранить виртуальные функции, например виртуальные маршрутизаторы или рут-рефлекторы. Можем добавить DNS-сервера, NTP, Syslog итд.

Веха 2. Система управления IP-пространством
Да, и в наше время находятся коллективы людей, которые ведут учёт префиксов и IP-адресов в Excel-файле. Но современный подход - это всё-таки база данных, с фронтендом на nginx/apache, API и широкими функциями по автоматическому выделению и учёту IP-адресов и сетей с разделением на VRF.
IPAM - IP Address Management.

Для наших задач в ней мы будем хранить следующую информацию:
    - VLAN'ы
    - VRF. 
    - Сети/Подсети/Агрегаты
    - IP-адреса
    - Привязка адресов к устройствам, сетей к локациям и номерам VLAN

Опять же понятно, что мы хотим быть уверены, что выделяя новый IP-адрес для лупбэка ToR'а мы не споткнёмся о то, что он уже был кому-то назначен. Или что один и тот же префикс мы использовали дважды в разных концах сети.
Но как это поможет в автоматизации?
Легко.
Запрашиваем в системе префикс, в котором есть доступные для выделения IP-адреса для лупбэков - если находится, выделяем адрес, если нет, запрашиваем создание нового префикса.
Или при создании конфигурации устройства мы из этой же системы можем узнать, в каком VRF должен находиться интерфейс. 
А при запуске нового сервера, Bootstrap скрипт !!!!! Сходит в систему, узнает в каком сервер свиче, в каком порту и какая подсеть назначена на интерфейс - из него и будет выделять адрес сервера.

Веха 3. Система описания сетевых сервисов
Если предыдущие две системы хранят переменные, которые ещё нужно как-то использовать, то третья описывает для каждой роли устройства, как она должна быть настроена.
Стоит выделить два разных типа сетевых сервисов: 
    - Инфраструктурные
    - Клиентские.

Первые призваны обеспечить базовую связность и управление устройством. Сюда можно отнести VTY, SNMP, NTP, Syslog, AAA, протоколы маршрутизации, CoPP итд.
Вторые организуют услугу для клиента: MPLS L2/L3VPN, GRE, VXLAN, VLAN, L2TP итд.
Разумеется, есть и пограничные случаи - куда отнести MPLS LDP, RSVP-TE? Да и протоколы маршрутизации могут использоваться для клиентов.

Оба типа сервисов раскладываются на конфигурационные примитивы:
    - физические и логические интерфейсы (тег/антег, mtu)
    - IP-интерфейсы (IP, IPv6, MPLS, VRF)
    - ACL и политики обработки трафика
    - IGP
    - BGP
    - Политики маршрутизации (префикс-листы, коммьюнити, ASN-фильтры).
    
Для описания сервисов мы будем использовать YANG, о котором в соответствующей части.

Если чуть ближе к жизни, то мы могли бы описать, что
Leaf-коммутатор должен иметь BGP-сессии со всем подключенными Spine-коммутаторами, импортировать в процесс подключенные сети, принимать от Spine-коммутаторов только сети из определённого префикса.
В свою очередь спайны держат сессии со всеми подключенными лифами, выступая в качестве рут-рефлекторов, и принимают от них только маршруты определённой длины.

Веха 4. Вендор-агностик конфигурационная модель
До сих пор все 3 системы были разрозненными лоскутами, дающими декларативное описание того, что мы хотели бы видеть на сети. Но рано или поздно, придётся иметь дело с конкретикой.
На этом этапе для каждого конкретного устройства примитивы, сервисы и переменные комбинируются в конфигурационную модель, фактически описывающую полную конфигурацию конкретного устройства, только в вендоронезависимой манере.
Что даёт этот шаг? Почему бы сразу не формировать конфигурацию устройства, которую можно просто залить?
Это позволяет решить три задачи:
    1) Не подстраиваться под конкретный интерфейс взаимодействия с устройством. Будь-то CLI, NETCONF, RESTCONF, SNMP - модель будет одинаковой.
    2) Не держать количество шаблонов/скриптов по числу вендоров в сети, и в случае изменения дизайна, менять одно и то же в нескольких местах.
    3) Загружать конфигурацию с устройства (бэкапа), раскладывать его в точно такую же модель и непосредственно сравнивать между собой целевую конфигурацию и имеющуюся для вычисления дельты и подготовки конфигурационного патча, который изменит только те части, которые необходимо.

В результате этого этапа мы получаем вендоронезависимую конфигурацию.

Веха 5. Вендор-специфик конфигурация.
Не стоит тешить себя надеждами на то, что когда-то настраивать циску можно будет точно так же, как джунипер, просто загрузив на них одинаковую конфигурацию. Несмотря на набирающие популярность whitebox'ы, на появление поддержки NETCONF, RESTCONF, OpenConfig конкретный контент, который этими протоколами доставляется, отличается от вендора к вендору, и это одно из их конкурентных отличий.
Примерно точно так же, ка OpenContrail и OpenStack, имеющие RestAPI в качестве своего NorthBound-интерфейса, ожидают совершенно разные вызовы. 
Итак, на пятом шаге вендоронезависимая модель должна принять ту форму, в которой она поедет на железо. 
И здесь все средства хороши (нет): CLI, NETCONF, RESTCONF, SNMP, простихоспаде.

Картинка.