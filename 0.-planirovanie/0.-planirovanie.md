Наверно, вы не раз слышали, что сеть всегда была самой инертной частью любой системы. И это правда во всех смыслах. 
Сеть - это базис, на который опирается всё, и производить изменения на ней довльно сложно - сервисы не терпят, когда сеть лежит. Зачастую вывод из эксплуатации одного узла может сложить большую часть приложений и повлиять на много клиентов.
Отчасти поэтому сетевая команда может сопротивляться любым изменениям - потому что сейчас оно как-то работает (мы не знаем как), а тут надо что-то новое настроить, и неизвестно как оно повлияет на сеть.

Сеть строится надолго. В современных сетях операторов легко можно увидеть устройства, у которых уже приближается десятилетний юбилей. А 10 лет назад мир был совсем другим (RFC NETCONF датирован 2011-м годом).  
Прописывание новых сервисов, даже если они хорошо известны, и есть шаблоны конфигурации, занимает много времени, потому что зачастую требуется индивидуальная настройка цепочки устройств.  
Автоматизации или нет совсем, или она достаточно примитивная. Поэтому даже если инженер не идёт руками в консоль, каждый узел в цеплочке настраивается отдельно.  
А человеку свойственно быть невнимательным и вообще ошибаться.  
И балом тут правят вендоры сетевого оборудования, у которых обычно закрытые чипы, предоставляющие только SDK, закрытое ПО, долгий срок решения проблем и добавления новых функций.   
Только при наличии единственного вендора на сети возможно использование проприетарных платформ, автоматизирующих конфигурацию.  
Настройка чего угодно в многовендорной сети - это всегда риск и эксперименты.

##Overlay

Чтобы не ждать, когда сетевики прокинут VLAN, люди придумали оверлеи - наложенные сети - коих великое многообразие.
GRE, IPinIP, MPLS, VxLAN, EVPN, MPLSoverUDP, MPLSoverGRE итд.

Оверлеи, которые начинаются на хосте - это вообще спасение - никаких изменений на андерлее (подлежащей сети) не требуется.
Хотя для этого требуется контроллер, который сообщит хостам, куда строить оверлейные туннели.

Здесь последние годы мы наблюдаем рост SDN - самые что ни на есть программно определяемые сети, ну совсем без людей.
Тут ключевые слова следующие: Openstack, OpenVswitch, Open Contrail (ныне Tungsten Fabric) - все они в той или иной мере реализуют идеи OpenFlow.

Однако подлежащие сети (будем называть их впредь адерлей) никуда не делись, они просто стали более статическими. Но задачи менять на них конфигурацию никто не отменял.

Да и новые сегменты сети не хочется настраивать вручную.

Даже если вы внедряете одну новую железку в месяц, вам нужна автоматизация.
<br>

Как это забавно, что во второй раз приходится проходить один и тот же путь.

Сначала пришлось писать самому статьи про сети из-за того, что их не было в рунете.

Теперь я не смог найти всесторонний документ, который систематизировал бы подходы к автоматизации и на простых практических примерах разбирал как работает NETCONF, что такое OpenConfig, как использовать ансибль, кто такие YANG'и, YAML', RestAPI.

Возможно, я ошибаюсь, поэтому, кидайте ссылки на годные ресурсы. Впрочем это не изменит моей решимости писать, потому что, основная цель - это всё-таки научиться чему-то самому, а облегчить жизнь ближнему - это приятный бонус, который ласкает ген распространения опыта.


Этой статьёй я начну серию о том, как _мне_ видится автоматизация.

* Поддержка целевого состояния
* Версионирование конфигурации сети
* Контролируемый откат изменений
* GIT
* CI в сети
* Система мониторинга сети




Тема эта сложная в первую очередь тем, что нельзя выработать универсальный фреймворк, который бы позволил заавтоматизировтаь всё и вся, иначе, я уверен, это было бы уже сделано.

К сожалению, автоматизация сети Интернет-провайдера кардинально отличается от автоматизации сети предприятия. А автоматизация сети предприятия отличается от ЦОДов.

В каждом из этих случаев требуется индивидуальный подход.

Я попытаюсь описать наиболее общие идеи, но на конкретных примерах.

И раз уж жизнь меня свела с Центрами Обработки Данных, то их и возьмём в качестве подопытных.


Мы возьмём средних размеров дата-центр Dcup и попробуем проработать всю схему автоматизации.

В Dcup 5 ДЦ, около 250 коммутаторов, полдюжины маршрутизаторов и пара файрволов.

Не фейсбук, но достаточно для того, чтобы глубоко задуматься об автоматизации. 

Бытует, впрочем, мнение, что если у вас больше 1 устройства, уже нужна автоматизация.

На самом деле тяжело представить, что кто-то сейчас может жить без неё или хотя бы пачки наколеночных скриптов.
Хотя я слышал, что есть такие конторы, где учёт IP-адресов ведётся в экселе, а каждое из тысяч сетевых устройств настраивается вручную и имеет свою неповторимую конфигурацию. Это, конечно, можно выдать за модерн-арт, но чувства инженера точно были оскорблены.

Итак, наша задача в этой серии выстроить систему, которая 
а) позволит настроить устройство в соответствии с его ролью и местоположением, когда оно добавляется в сеть. 
б) будет поддерживать актуальное и консистентное состояние конфигурации на всей сети.

Мы минимизируем вплоть до нуля хождение в CLI руками - любые изменения в настройках устройств или дизайне сети должны быть формализованы и документированы - и только потом выкатываться на нужные элементы сети.


> То есть например, если мы решили, что с этого момента стоечные коммутаторы в Казани!!! должны анонсировать две сети вместе одной, мы
>    1) Сначала документируем изменения в системах
>    2) Готовим целевую модель конфигурации
>    3) Запускаем программу обновления конфигурации сети, которая вычисляет, что нужно удалить на каждом узле, что добавить, и приводит узлы к нужному состоянию.
> Это вместо того, чтобы на каждое устройство идти руками, пусть даже с подготовленной копипастой.

Ещё одна идея, которая естественным образом вытекает из поддержки состояния на всей сети - это версионирование конфигурации - каждое новое изменение на сети, будь то глобальные изменения в политиках маршрутизации или добавление VLAN'а на порт одного устройства, - это новая версия сети. 

> Например, мы задокументировали сетевой дизайн Dcup и назначили ему версию 1.0. Автоматика раскатила версию 1.0 на все узлы, приведя сеть в целевое состояние.
> Потом мы решили добавить новый NTP-сервер на все устройства, стоящие в Казанском ДЦ. Мы решили, что это минорное изменение и назначили этому дизайну версию 1.01. Автоматика раскатила версию 1.0 на все узлы, приведя сеть в целевое состояние.
> Ещё через какое-то время мы решили поменять маршрутную политику с BGP-пирами. Это более значимое изменение - присвоили ему версию 1.1. Автоматика раскатила версию 1.0 на все узлы, приведя сеть в целевое состояние.
> Вдруг через 2 минуты системы мониторинга начинают выть, предупреждая, что у нас в Казани недостача важных маршрутов. Оказалось, что в политике допустили досадную ошибку.

#### Откат
Что бы мы делали в обычной ситуации?
Если устройство поддерживает коммиты, то, вероятно, сделали бы на каждом устройстве откат конфигурации до предыдущего коммита. 
Если же нет - сформировали бы набор конфигурационных патчей, которые отменяли бы последние изменения и применили их вручную на каждом устройстве, пытаясь не перепутать какой патч куда применить. 

В нашей новой парадигме мы приводим сеть к состоянию, согласно версии конфигурации 1.01.
То есть это фактически не откат изменений, а приведение сети к новому состоянию.

От инженера не требуется ничего, кроме того, чтобы одним нажатием запустить на всей сети новую (старую) конфигурацию. А автоматика сама разберётся какие именно команды и куда применять.




В описываемых тут идеях я буду не оригинален. У Дмитрия Фиголя!!! есть отличный канал со стримами на эту тему.
Статьи во многих аспектах будут с ними пересекаться.

И как и в случае с СДСМ начнём мы с очень общих представлений и подхода к планированию. 
В качестве основы я выбрал один из современных дизайнов датацентровой сети - L3 Clos Fabric с BGP в качестве протокола маршрутизации.
Строить сеть в лабе мы будем на этот раз на Juniper. 
Подробнее о сетевом дизайне в следующей статье!!!


Итак, наша цель - поддержка сети в нужном состоянии, а если быть точнее, то обеспечение всего жизненного цикла конфигурации:
Иметь список всех устройств в сети, их расположения, ролей, моделей, версий ПО.
Создавать начальный конфиг.
Обновлять его до актуальной версии при изменениях.
Откатывать конфиг до предыдущей (или произвольной) версии.
Периодически проверять все устройства на предмет отхождения от актуального (ночью кто-то тихонько добавил правило в ACL) и приводить к актуальному.
Делать бэкапы конфигурации.

Звучит достаточно сложно для того, чтобы начать декомпозировать проект на вехи.

И будет их шесть:

Инвентарная система, 
Система управления IP-пространством, 
Система описания сетевых сервисов
Вендор-агностик конфигурационная модель
Вендор-специфик драйвер.
Применение конфигурации.

> Поскольку мы в самом начале пути, исходная концепция может пересматриваться в будущем.



### Веха 1. Инвентарная система
Очевидно, мы хотим знать, какое оборудование, где стоит, к чему подключено.
Инвентарная система - неотъемлемая часть любого предприятия. 
Однако гораздо полезнее вести учёт не просто единицам оборудования, а всей инфраструктуре, поднявшись на уровень абстрации выше: что и где стоит, как друг к другу подключено, какие порты в какие патч-панели скоммутированы, какие плечи питания подведены.

Это всё функции DCIM - Data Center Infrastructure Management.

Для наших задач в ней мы будем хранить следующую информацию про устройство:
* Инвентарный номер
* Название/описание
* Тип (коммутатор, маршрутизатор, сервер итд)
* Модель (Huawei CE12800, Juniper QFX5200 итд)
* Характерные параметры (платы, интерфейсы итд)
* Роль (Leaf, Spine, Border Router итд)
* Локацию (регион, город, дата-центр, стойка, юнит)
* Интерконнекты между устройствами
* Топологию сети


Прекрасно понятно, что нам самим хочется знать всё это. 
Но поможет ли это в целях автоматизации?
Безусловно.
Например, мы знаем, что в данном дата-центре на Leaf-коммутаторах, если это Huawei, ACL для фильтрации определённого трафика должны применяться на VLAN, а если это Juniper, то на IRB.
Или нужно раскатить новый SYSlog-сервер на все сетевые устройства одного определённого региона.

В ней же мы будем хранить виртуальные функции, например виртуальные маршрутизаторы или рут-рефлекторы. Можем добавить DNS-сервера, NTP, Syslog итд.

Требование к DCIM-системе - она должна иметь API, через который можно читать и записывать информацию.

### Веха 2. Система управления IP-пространством
Да, и в наше время находятся коллективы людей, которые ведут учёт префиксов и IP-адресов в Excel-файле. Но современный подход - это всё-таки база данных, с фронтендом на nginx/apache, API и широкими функциями по автоматическому выделению и учёту IP-адресов и сетей с разделением на VRF.
Вторая неотъемлемая часть любой современной сети - IPAM - IP Address Management.

Для наших задач в ней мы будем хранить следующую информацию:
* VLAN'ы
* VRF
* Сети/Подсети/Агрегаты
* IP-адреса
* Привязка адресов к устройствам, сетей к локациям и номерам VLAN

Опять же понятно, что мы хотим быть уверены, что выделяя новый IP-адрес для лупбэка ToR'а мы не споткнёмся о то, что он уже был кому-то назначен. Или что один и тот же префикс мы использовали дважды в разных концах сети.
Но поможет ли это в целях автоматизации?
Безусловно.
Запрашиваем в системе префикс, в котором есть доступные для выделения IP-адреса для лупбэков - если находится, выделяем адрес, если нет, запрашиваем создание нового префикса.
Или при создании конфигурации устройства мы из этой же системы можем узнать, в каком VRF должен находиться интерфейс. 
А при запуске нового сервера, Bootstrap-скрипт cходит в систему, узнает в какой стойке стоит сервер, какой в этой стойке ToR-свич и какая подсеть назначена на L3-интерфейс - из него и будет выделяться адрес серверу.

API - так же обязательное требование и к IPAM-системе.

### Веха 3. Система описания сетевых сервисов
Если предыдущие две системы просто хранят переменные, то третья описывает как каждое устройство должно быть настроено.
Стоит выделить два разных типа сетевых сервисов: 
    - Инфраструктурные
    - Клиентские.

Первые призваны обеспечить базовую связность и управление устройством. Сюда можно отнести VTY, SNMP, NTP, Syslog, AAA, CoPP итд.
Вторые организуют услугу для клиента: MPLS L2/L3VPN, GRE, VXLAN, VLAN, L2TP итд.
Разумеется, есть и пограничные случаи - например, протоколы маршрутизации.

Оба типа сервисов раскладываются на конфигурационные примитивы:
    - физические и логические интерфейсы (тег/антег, mtu)
    - IP-интерфейсы (IP, IPv6, MPLS, VRF)
    - ACL и политики обработки трафика
    - IGP
    - BGP
    - Политики маршрутизации (префикс-листы, коммьюнити, ASN-фильтры).
    
Для описания сервисов мы будем использовать _какую-нибудь_ модель, о чём поговорим в соответствующей части.

Если чуть ближе к жизни, то мы могли бы описать, что
Leaf-коммутатор должен иметь BGP-сессии со всем подключенными Spine-коммутаторами, импортировать в процесс подключенные сети, принимать от Spine-коммутаторов только сети из определённого префикса.
В свою очередь спайны держат сессии со всеми подключенными лифами, выступая в качестве рут-рефлекторов, и принимают от них только маршруты определённой длины.

### Веха 4. Вендор-агностик конфигурационная модель
До сих пор все 3 системы были разрозненными лоскутами, дающими декларативное описание того, что мы хотели бы видеть на сети. Но рано или поздно, придётся иметь дело с конкретикой.
На этом этапе для каждого конкретного устройства примитивы, сервисы и переменные комбинируются в конфигурационную модель, фактически описывающую полную конфигурацию конкретного устройства, только в вендоронезависимой манере.
Что даёт этот шаг? Почему бы сразу не формировать конфигурацию устройства, которую можно просто залить?
Это позволяет решить три задачи:
    1) Не подстраиваться под конкретный интерфейс взаимодействия с устройством. Будь-то CLI, NETCONF, RESTCONF, SNMP - модель будет одинаковой.
    2) Не держать количество шаблонов/скриптов по числу вендоров в сети, и в случае изменения дизайна, менять одно и то же в нескольких местах.
    3) Загружать конфигурацию с устройства (бэкапа), раскладывать его в точно такую же модель и непосредственно сравнивать между собой целевую конфигурацию и имеющуюся для вычисления дельты и подготовки конфигурационного патча, который изменит только те части, которые необходимо.

В результате этого этапа мы получаем вендоронезависимую конфигурацию.

### Веха 5. Вендор-специфик конфигурация.
Не стоит тешить себя надеждами на то, что когда-то настраивать циску можно будет точно так же, как джунипер, просто загрузив на них одинаковую конфигурацию. Несмотря на набирающие популярность whitebox'ы, на появление поддержки NETCONF, RESTCONF, OpenConfig конкретный контент, который этими протоколами доставляется, отличается от вендора к вендору. Это одно из их конкурентных отличий, с которым они ещё долго будут не готовы расстаться.
Примерно точно так же, ка OpenContrail и OpenStack, имеющие RestAPI в качестве своего NorthBound-интерфейса, ожидают совершенно разные вызовы. 
Итак, на пятом шаге вендоронезависимая модель должна принять ту форму, в которой она поедет на железо. 
И здесь все средства хороши (нет): CLI, NETCONF, RESTCONF, SNMP простихоспаде.

В соответствующей части мы разберёмся, как это сделать.

Картинка.



#### GIT
GIT уверенно шагает по IT.
Для наших задач мы можем применить его как минимум в двух местах.
Во-первых, конечно версионирование дизайна. Любые изменения в конфигурации сети мы можем хранить в гите и давать им версии.
Во-вторых, бэкап конфигурации. GIT позволяет хранить сколь угодно старые конфигурации, не увеличивая линейно размер архива, благодря тому что при любом изменении он хранить только разницу. И при этом в любой момент можно вернуться к нужному моменту в истории.

#### CI в сети
Вторая вещь, которую мы можем перетащить с экстремальной пользой из сферы разработки в сеть - это Continious Integration. 

Новая версия конфигурации сначала автоматикой накатывается на тестовый стенд, который полностью повторяет собой рабочую сеть. 
Во-первых, проверяется, что конфигурация в принципе принимается и применяется.
Во-вторых, что после её применения ничего не ломается. Об этом сообщат системы мониторинга и проактивные тесты, которые прогоняются сразу после выкатки новой версии.
В-третьих, версия вызревает на тестовом стенде в течение нескольких часов-дней, доказывая свою работоспособность и безопасность.

Далее проверенная версия выкатывается в рабочую сеть, где происходит то же самое: прогоняются проактивные тесты, а система мониторинга следит за здоровьем сети.

#### Мониторинг
Система слежения за состоянием сети - это настолько важная часть, что без неё автоматизация практически немыслима. 
Всем ведь известно, что автоматизация позволяет очень малыми усилиями вывести из строя максимально большое числов устройств!!!!
Важно быстро обнаружить где болит и выписать таблетку.
Однако понять, что и где мониторить, на какие события регистрировать предупреждение, как реагировать - это сложнейшая задача.
Например, мы могли бы проверять общее количество полученных по BGP префиксов и начинать кричать, когда он сильно меняется.
Или выстроить бейзлайн объёма трафика из ДЦ и зажигать лампочку, если его стало внезапно меньше.

В будущих статьях мы ещё поговорим про правильный мониторинг.
