.. meta::
   :http-equiv=Content-Type: text/html; charset=utf-8

Виртуальная коммутация
======================

Если есть виртуальная машина, а в ней есть виртуальный интерфейс, то, очевидно, возникает задача передачи пакета из одной VM в другую. В Linux-based гипервизорах (KVM, например) эта задача может решаться с помощью Linux bridge, однако, большое распространение получил проект `Open vSwitch <https://www.openvswitch.org>`_(OVS).
Есть несколько основных функциональностей, которые позволили OVS широко распространиться и стать de-facto основным методом коммутации пакетов, который используется во многих платформах облачных вычислений(например, Openstack) и виртуализированных решениях.

* Передача сетевого состояния - при миграции VM между гипервизорами возникает задача передачи ACL, QoSs, L2/L3 forwarding-таблиц и прочего. И OVS умеет это.
* Реализация механизма передачи пакетов (datapath) как в kernel, так и в user-space
* CUPS (Control/User-plane separation) архитектура - позволяет перенести функциональность обработки пакетов на специализированный chipset (Broadcom и Marvell chipset, например, могут такое), управляя им через control-plane OVS.
* Поддержка методов удаленного управления трафиком - протокол OpenFlow (привет, SDN).

Архитектура OVS на первый взгляд выглядит довольно страшно, но это только на первый взгляд.

    .. figure:: https://fs.linkmeup.ru/images/adsm/1/1/ovs_architecture_01.png
           :width: 800
           :align: center


Для работы с OVS нужно понимать следующее:

* **Datapath** - тут обрабатываются пакеты. Аналогия - switch-fabric железного коммутатора. Datapath включает в себя приём пакетов, обработку заголовков, поиск соответствий по таблице flow, который в Datapath уже запрограммирован. Если OVS работает в kernel, то выполнен в виде модуля ядра. Если OVS работает в user-space, то это процесс в user-space Linux.
* **vswitchd** и **ovsdb** - демоны в user-space, то что реализует непосредственно сам функциональность коммутатора, хранит конфигурацию, устанавливает flow в datapath и программирует его.
* Набор инструментов для настройки и траблшутинга OVS - **ovs-vsctl, ovs-dpctl, ovs-ofctl, ovs-appctl**. Все то, что нужно, чтобы прописать в ovsdb конфигурацию портов, прописать какой flow куда должен коммутироваться, собрать статистику и прочее. Добрые люди `написали статью <http://therandomsecurityguy.com/openvswitch-cheat-sheet/>`_ по этому поводу.

**Каким же образом сетевое устройство виртуальной машины оказывается в OVS?**

Для решения данной задачи нам необходимо каким-то образом связать между собой виртуальный интерфейс, находящийся в user-space операционной системы с datapath OVS, находящимся в kernel.

В операционной системе Linux передача пакетов между kernel и user-space-процессами осуществляется посредством двух специальных интерфейсов. Оба интерфейса использует запись/чтение пакета в/из специальный файл для передачи пакетов из user-space-процесса в kernel и обратно - file descriptor (FD) (это одна из причин низкой производительности виртуальной коммутации, если datapath OVS находится в kernel - каждый пакет требуется записать/прочесть через FD)

* **TUN** (tunnel) - устройство, работающее в L3 режиме и позволяющее записывать/считывать только IP пакеты в/из FD.
* **TAP** (network tap) - то же самое, что и tun интерфейс + умеет производить операции с Ethernet-фреймами, т.е. работать в режиме L2.
</ul>

    .. figure:: https://fs.linkmeup.ru/images/adsm/1/1/virtual-devices-all.png
           :width: 800
           :align: center

Именно поэтому при запущенной виртуальной машине в Host OS можно увидеть созданные TAP-интерфейсы командой *ip link* или *ifconfig* - это "ответная" часть virtio, которая "видна" в kernel Host OS. Также стоит обратить внимание, что TAP-интерфейс имеет тот же MAC-адрес что и virtio-интерфейс в виртуальной машине.

TAP-интерфейс может быть добавлен в OVS с помощью команд *ovs-vsctl* - тогда любой пакет, скоммутированный OVS в TAP-интерфейс, будет передан в виртуальную машину через file descriptor.

    Реальный порядок действий при создании виртуальной машины может быть разным, т.е. сначала можно создать OVS bridge, потом указать виртуальной машине создать интерфейс, соединенный с этим OVS, а можно и наоборот.

Теперь, если нам необходимо получить возможность передачи пакетов между двумя и более виртуальными машинами, которые запущены на одном гипервизоре, нам потребуется лишь создать OVS bridge и добавить в него TAP-интерфейсы с помощью команд ovs-vsctl. Какие именно команды для этого нужны легко гуглится.

На гипервизоре может быть несколько OVS bridges, например, так работает Openstack Neutron, или же виртуальные машины могут находиться в разных namespace для реализации multi-tenancy.

**А если виртуальные машины находятся в разных OVS bridges?**

Для решения данной задачи существует другой инструмент - **veth pair**. Veth pair может быть представлен как пара сетевых интерфейсов, соединенных кабелем - все то, что "влетает" в один интерфейс, "вылетает" из другого. Veth pair используется для соединения между собой нескольких OVS bridges или Linux bridges. Другой важный момент что части veth pair могут находиться в разных namespace Linux OS, то есть veth pair может быть также использован для связи namespace между собой на сетевом уровне.
