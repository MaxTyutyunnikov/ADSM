В первых двух статьях я поднял вопрос автоматизации и набросал её фреймворк, во второй сделал отступление в виртуализацию сети, как первый подход к автоматизации настройки сервисов.
А теперь пришло время нарисовать схему физической сети.

Описанные в этой серии практики должны быть применимы к сети любого типа, любого масштаба с любым многообразием вендоров (нет).

Однако нельзя описать универсальный пример применения этих подходов. Поэтому я остановлюсь на современной архитектуре сети ДЦ: Фабрике Клоза.
DCI сделаем на  MPLS L3VPN.
Поверх физической сети работает Overlay-сеть с хоста (это может быть VXLAN OpenStack'а или Tungsten Fabric или что угодно другое, что требует от сети только базовой IP-связности.)

<img src="https://fs.linkmeup.ru/images/adsm/2/bird_view.png" width="800">

В этом случае получится сравнительно простой сценарий для автоматизации, потому что имеем много оборудования, настраивающегося одинаковым образом. 
Мы выберем сферический ДЦ в вакууме:
    - Одна версия дизайна везде. 
    - Два вендора, образующих две плоскости сети.
    - Один ДЦ похож на другой как две капли воды.

Содержание:
<ul>
    <li><a href="#TOPOLOGY">Физическая топология</a></li>
    <li><a href="#ROUTING">Маршрутизация</a></li>
    <ul>
        <li><a href="#SESSIONS">Сессии и политики</a></li>
        <li><a href="#ASNS">BGP ASN</a></li>
    </ul>
    <li><a href="#IP_PLAN">IP-план</a></li>
    <li><a href="#REALITY">Лаба</a></li>
    <li><a href="#THEEND">Заключение</a></li>
    <li><a href="#LINKS">Полезные ссылки</a></li>
</ul>

Пусть наш Сервис-Провайдер будет, например, хостить обучающие видео о выживании в застрявших лифтах.
В мегаполисах это пользуется бешенной популярностью, поэтому физических машин надо много.

Сначала я опишу сеть приблизительно такой, какой бы её хотелось видеть. А потом упрощу для лабы.
<hr>

<a name="TOPOLOGY"></a>
<h1>Физическая топология</h1>
Локации
У LAN_DC будет 6 ДЦ: 
<ul>
    <li>Россия (<b>RU</b>):
    <ul>
        <li>Москва (<b>msk</b>)</li>
        <li>Казань (<b>kzn</b>)</li>
    </ul>
    </li>
    <li>Испания (<b>SP</b>):
    <ul>
        <li>Барселона (<b>bcn</b>)</li>
        <li>Малага (<b>mlg</b>)</li>
    </ul>
    </li>
    <li>Китай (<b>CN</b>):
    <ul>
        <li>Шанхай (<b>sha</b>)</li>
        <li>Сиань (<b>sia</b>)</li>
    </ul>
    </li>
</ul>

<img src="https://fs.linkmeup.ru/images/adsm/2/locations.png" width="800">

<h3>Внутри ДЦ</h3>
Во всех ДЦ идентичные сети внутренней связности, основанные на сетях Клоза. 
Что за сети Клоза и почему именно они - в отдельной статье-заметке!!!

В каждом ДЦ по 10 стоек с машинами, они будут нумероваться как <b>A</b>, <b>B</b>, <b>C</b> итд.
В каждой стойке по 30 машин. Они нас интересовать не будут.
Также в каждой стойке стоит коммутатор, к которому подключены все машины - это Top of the Rack switch - ToR или иначе в терминах фабрики Клоза будем называть его Leaf.

<img src="https://fs.linkmeup.ru/images/adsm/2/fabric.png" width="800">
<i>Общая схема фабрики.</i>

Именовать их будем <i><b>XXX</b>-leaf<b>Y</b></i>, где <b>XXX</b> - трёхбуквенное сокращение ДЦ, а <b>Y</b> - порядковый номер. Например, <i>kzn-leaf11</i>.


<blockquote>
    Я в статьях позволю себе достаточно фривольно обращаться терминами Leaf и ToR, как синонимами. Однако нужно помнить, что это не так.
    ToR - это конкретная роль устройства в физической топологии.
    Leaf - это конечное устройство в терминах Фабрики Клоза.
    Так Leaf'ом может быть EndofRaw-коммутатор, например.
</blockquote>

Каждый ToR-коммутатор в свою очередь соединён с четырьмя вышестоящими агрегирующими коммутаторами - Spine. Под Spine'ы выделено по одной стойке в ДЦ. Именовать будем аналогично: <i><b>XXX</b>-spine<b>Y</b></i>.

В этой же стойке будет стоять сетевое оборудование для связности между ДЦ - 2 маршрутизатора с MPLS на борту. Но по большому счёту - это те же самые ToR'ы. То есть с точки зрения Spine-коммутаторов не имеет никакого значения обычный там ToR с подключенными машинами или маршрутизатор для DCI - один чёрт форвардить.
Такие специальные ToR'ы называются Edge-leaf. Мы их будем именовать <i><b>XXX</b>-edge<b>Y</b></i>.

Троица Leaf, Spine и Edge образуют Underlay-сеть.

Задача сетевой фабрики, как мы уже определились в <a href="https://linkmeup.ru/blog/449.html">прошлом выпуске</a>, очень и очень простая - обеспечить IP-связность между машинами как в пределах одного ДЦ, так и между. 
Оттого-то сеть и называется фабрикой, так же, например, как  фабрика коммутации внутри модульных сетевых коробок, о чём подробнее можно почитать в <a href="https://linkmeup.ru/blog/312.html">СДСМ14</a>.

Фабрика полностью L3. Никаких VLAN, никаких Broadcast - вот такие у нас в LAN_DC замечательные программисты, умеют писать приложения, живущие в парадигме L3, а виртуальные машины не требуют Live Migration c сохранением IP-адреса.

И ещё раз ответ на вопрос почему фабрика и почему L3 - в отдельной статье?!!!

<h3>DCI - Data Center Interconnect</h3>
DCI будет организован с помощью Edge-Leaf - то есть они - наша точка выхода в магистраль.
Для простоты предположим, что ДЦ связаны между собой прямыми линками.
Исключим из рассмотрения внешнюю связность.
<blockquote>
    Я отдаю себе отчёт в том, что каждый раз, как я убираю какой-либо компонент, я значительно упрощаю сеть. И при автоматизации нашей абстрактной сети всё будет хорошо, а на реальной возникнут костыли.
    Это так. И всё же задача этой серии - подумать и поработать над подходами, а не героически решать выдуманные проблемы.
</blockquote>
На Edge-Leaf'ах  underlay помещается в VPN и передаётся через MPLS-магистраль (тот самый прямой линк).

Вот такая верхнеуровневая схема получается.

<img src="https://fs.linkmeup.ru/images/adsm/2/network.png" width="800">

<hr>

<a name="ROUTING"></a>
<h1>Маршрутизация</h1>

Для маршрутизации внутри ДЦ будем использовать BGP. 
На MPLS-магистрали OSPF+LDP.
Для DCI, то есть организации связности в андерлее - BGP L3VPN over MPLS.

<img src="https://fs.linkmeup.ru/images/adsm/2/bird_view.png" width="800">
<i>Общая схема маршрутизации</i>

На фабрике никаких OSPF и ISIS (запрещённый в Российской Федерации протокол маршрутизации).
А это значит, что не будет Auto-discovery и вычисления кратчайших путей.
Ручная (нет - автоматическая) настройка протокола, соседства и политик.

<img src="https://fs.linkmeup.ru//images/adsm/2/bgp_in_dc.png" width="600">
<i>Схема маршрутизации BGP внутри ДЦ</i>

Почему BGP?
На эту тему есть <a href="https://tools.ietf.org/html/rfc7938" target="_blank">целый RFC</a> имени Facebook'a и Arista, где рассказывается, как строить <b>очень крупные</b> сети датацентров, используя BGP. Читается почти как художественный, очень рекомендую для томного вечера.
В нём и про Clos-топологии, и про L3-дизайны, и про BGP в качестве единственного протокола маршрутизации, и даже про OPEX с CAPEX'ом. В общем мне Лапухов (он один из авторов) идею продал. 

Что такое крупный - это тысячи стоек в одном датацентре, то есть это тысячи сетевых устройств. Даже при разбиении на OSPF-зоны или ISIS (запрещённый в Российской Федерации протокол маршрутизации)-level'ы проблема работы Link-State протоколов вопросы остаются: политики маршрутизации, агрегация маршрутов, а если у нас VRF, а если хочется распространять не только unicast маршруты?
При этом от BGP как протокола сигнализации между датацентрами, скорее всего, никуда не денешься.
А BGP супер-масштабируемый, расширяемый протокол, который уже показал свою эффективность на самой крупной сети нашей Солнечной системы.
Десятки и сотни тысяч (даже миллионы) маршрутов он ест на завтрак.
Как раз то, что нужно. 
Тем более мы сокращаем стек используемых технологий - везде BGP.
С точки зрения маршрутизации один датацентр можно считать единым доменом.

<i>Стоит вспомнить, как в середине СДСМ мы наивно смеялись над идеей BGP в качестве IGP, а теперь пришлось к ней обратиться</i>.

<blockquote>
    Положа руку на сердце, на нашей фабрике, которая с большой долей вероятности не будет стремительно расти, за глаза хватило бы и OSPF. Это на самом деле проблемы мегаскейлеров и клауд-титанов. Но пофантазируем всего лишь на несколько выпусков, что нам это нужно, и будем использовать BGP, как завещал Пётр Лапухов.
</blockquote>
<hr>
<a name="SESSIONS"></a>
<h2>Сессии и политики</h2>
 На Leaf-коммутаторах мы импортируем в BGP префиксы с Underlay'ных интерфейсов с сетями.
У нас будет BGP-сессия между <b>каждой</b> парой Leaf-Spine, в которой эти Underlay'ные префиксы будут анонсироваться по сети тудыть-сюдыть.

Внутри одного датацентра мы будем распространять специфики, которые импортировали на ТоРе. На Edge-Leaf'ах будем их агрегировать и анонсировать в удалённые ДЦ и спускать до ТоРов. То есть каждый ТоР будет знать точно, как добраться до другого ТоРа в этом же ДЦ и где точка выхода, чтобы добраться до ТоРа в другом ДЦ.
В DCI маршруты будут передаваться, как VPNv4. Для этого на Edge-Leaf интерфейс в сторону фабрики будет помещаться в VRF, назовём его UNDERLAY, и соседство со Spine на Edge-Leaf будет подниматься внутри VRF, а между Edge-Leaf'ами в VPNv4-family.

<img src="https://fs.linkmeup.ru/images/adsm/2/routing.png" width="800">

А ещё мы запретим реанонсировать маршруты полученные от вышестоящего уровня, обратно на него.

<img src="https://fs.linkmeup.ru/images/adsm/2/no_reannounce.png" width="500">

На Leaf и Spine мы не будем импортировать Loopback'и. Они нам понадобятся только для того, чтобы определить Router ID. 
А вот на Edge-Leaf'ах импортируем его в Global BGP. Между Loopback-адресами Edge-Leaf'ы будут устанавливать BGP-сессию в IPv4 VPN-family друг с другом.

Между EDGE-устройствами у нас будет растянута магистраль на OSPF+LDP.  Всё в одной зоне. Предельно простая конфигурация.

Вот такая картина с маршрутизацией.
<hr>
<a name="ASNS"></a>
<h2>BGP ASN</h2>
<h3>Edge-Leaf ASN</h3>
На Edge-Leaf'ах будет один ASN во всех ДЦ. Это важно, чтобы между Edge-Leaf'ами был iBGP, и мы не накололись на нюансы eBGP. Пусть это будет 65535. В реальности это мог бы быть номер публичной AS.


<h3>Spine ASN</h3>
На Spine у нас будет один ASN на ДЦ. Начнём здесь с самого первого номера из диапазона приватных AS - 64512, 64513 итд.
Почему ASN на ДЦ? 
Декомпозируем этот вопрос на два: 
Почему одинаковые ASN на всех Спайнах одного ДЦ,
Почему разные в разных ДЦ

<b>Почему одинаковые ASN на всех Спайнах одного ДЦ</b>
Вот как будет выглядеть AS-Path Андерлейного маршрута на Edge-Leaf:
[leaf1_ASN, <b>spine_ASN</b>, edge-leaf_ASN]
При попытке заанонсировать его обратно на Спайн, тот его отбросит потому что его AS (Spine_AS) уже есть в списке. 

Однако в пределах ДЦ нас совершенно устраивает, что маршруты Underlay, поднявшиеся до Edge не смогут спуститься вниз. Вся коммуникация между хостами внутри ДЦ должна происходить в пределах Спайнов. 
<img src="https://fs.linkmeup.ru/images/adsm/2/as_path_intra_dc.png" width="800">

При этом агрегированные маршруты других ДЦ в любом случае беспрепятственно будут доходить до ТоРов - в их AS-Path будет только ASN 65535 - номер AS Edge-Leaf'ов, потому что именно на них они были созданы.

<b>Почему разные в разных ДЦ</b>
Теоретически нам может потребоваться протащить Loopback'и каких-нибудь сервисных виртуальных машин между ДЦ.
Например, на хосте у нас запустится Route Reflector или тот самый VNGW (Virtual Network Gateway), который по BGP запирится с ТоРом и проанонсирует свой Лупбэк, который должен быть доступен из всех ДЦ. 
Так вот как будет выглядеть его AS-Path:
[VNF_ASN, leaf_DC1_ASN, <b>spine_DC1_ASN</b>, edge-leaf_ASN, <b>spine_DC2_ASN</b>, leaf_DC2_ASN].
И здесь нигде не должно быть повторяющихся ASN. 
То есть Spine_DC1 и Spine_DC2 должны быть разными, ровно как и ToR_DC1 и ToR_DC2, к чему мы как раз и подходим.
<blockquote>
    Как вы, наверно, знаете, существуют хаки, позволяющие принимать маршруты с повторяющимися ASN вопреки механизму предотвращения петель (allowas-in на Cisco). И у этого есть даже вполне законные применения. Но это потенциальная брешь в устойчивости сети. 
    И если у нас есть возможность не использовать опасные вещи, мы ей воспользуемся.
</blockquote>

<h3>Leaf ASN</h3>
У нас будет индивидуальный ASN на каждый Leaf-коммутаторах в пределах всей сети.
Делаем мы так из соображений, приведённых выше: AS-Path без петель, конфигурация BGP без закладок.
Чтобы маршруты между Leaf'ами беспрепятственно проходили, AS-Path должен выглядеть так:
[leaf1_ASN, spine_ASN, leaf2_ASN]
где leaf1_ASN и leaf2_ASN хорошо бы, чтобы отличались.
Требуется это и для ситуации с анонсом Лупбэка VNF между ДЦ:
[VNF_ASN, <b>leaf_DC1_ASN</b>, spine_DC1_ASN, edge-leaf_ASN, spine_DC2_ASN, <b>leaf_DC2_ASN</b>]

Будем использовать 4-байтовый ASN и генерировать его на основе ASN Spine'а и номера Leaf-коммутатора, а именно, вот так: <i>Spine_ASN.0000X</i>.


Вот такая картина с ASN.
<img src="https://fs.linkmeup.ru/images/adsm/2/asns.png" width="800">
<hr>

<a name="IP_PLAN"></a>
<h1>IP-план</h1>

Принципиально, нам нужно выделить адреса для следующих подключений:
<ol>
    <li>Адреса сети Underlay между ToR и машиной. Они должны быть уникальны в пределах всей сети, чтобы любая машина могла связаться с любой другой. Отлично подходит <b>10/8</b>. На каждую стойку по /26 с запасом. Будем выделять по /19 на ДЦ и /17 на регион.
    </li>
    <li>Линковые адреса между Leaf/Tor и Spine. 
    Их хотелось бы назначать алгоритмически, то есть вычислять из имён устройств, которые нужно подключить.
    Пусть это будет… 169.254.0.0/16. 
    А именно <b>169.254.00X.Y/31</b>, где <b>X</b> - номер Spine, <b>Y</b> - P2P-сеть /31.
    Это позволит запускать до 128 стоек, и до 10 Spine в ДЦ. Линковые адреса могут (и будут) повторяться из ДЦ в ДЦ.</li>
    <li>Cтык Spine - Edge-Leaf организуем на подсетях <b>169.254.10X.Y/31</b>, где точно так же <b>X</b> - номер Spine, <b>Y</b> - P2P-сеть /31.</li>
    <li>Линковые адреса из Edge-Leaf в MPLS-магистраль. Здесь ситуация несколько иная - место соединения всех кусков в один пирог, поэтому переиспользовать те же самые адреса  не получится - нужно выбирать следующую свободную подсеть. Поэтому за основу возьмём <b>192.168.0.0/16</b> и будем из неё выгребать свободные.</li>
    <li>Адреса Loopback.  Отдадим под них весь диапазон <b>172.16.0.0/12</b>.
    <ul>
        <li>Leaf - по /25 на ДЦ - те же 128 стоек. Выделим по /23 на регион.</li>
        <li>Spine - по /28 на ДЦ - до 16 Spine. Выделим по /26 на регион.</li>
        <li>Edge-Leaf - по /29 на ДЦ - до 8 коробок.  Выделим по /27 на регион.</li>
    </ul>
    </li>
</ol>

Вот такая картина с IP-адресацией.
<img src="https://fs.linkmeup.ru/images/adsm/2/ip_plan.png" width="800">

Loopback'и:
<table border="2">
    <tr>
        <td><b>Префикс</b></td>
        <td><b>Роль устройства</b></td>
        <td><b>Регион</b></td>
        <td><b>ДЦ</b></td>
    </tr>
    <tr>
        <td>172.16.0.0/23</td>
        <td rowspan="10">edge</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.0.0/27</td>
        <td rowspan="3">ru</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.0.0/29</td>
        <td>msk</td>
    </tr>
    <tr>
        <td>172.16.0.8/29</td>
        <td>kzn</td>
    </tr>
    <tr>
        <td>172.16.0.32/27</td>
        <td rowspan="3">sp</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.0.32/29</td>
        <td>bcn</td>
    </tr>
    <tr>
        <td>172.16.0.40/29</td>
        <td>mlg</td>
    </tr>
    <tr>
        <td>172.16.0.64/27</td>
        <td rowspan="3">cn</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.0.64/29</td>
        <td>sha</td>
    </tr>
    <tr>
        <td>172.16.0.72/29</td>
        <td>sia</td>
    </tr>
    <tr>
        <td>172.16.2.0/23</td>
        <td rowspan="10">spine</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.2.0/26</td>
        <td rowspan="3">ru</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.2.0/28</td>
        <td>msk</td>
    </tr>
    <tr>
        <td>172.16.2.16/28</td>
        <td>kzn</td>
    </tr>
    <tr>
        <td>172.16.2.64/26</td>
        <td rowspan="3">sp</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.2.64/28</td>
        <td>bcn</td>
    </tr>
    <tr>
        <td>172.16.2.80/28</td>
        <td>mlg</td>
    </tr>
    <tr>
        <td>172.16.2.128/26</td>
        <td rowspan="3">cn</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.2.128/28</td>
        <td>sha</td>
    </tr>
    <tr>
        <td>172.16.2.144/28</td>
        <td>sia</td>
    </tr>
    <tr>
        <td>172.16.8.0/21</td>
        <td rowspan="10">leaf</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.8.0/23</td>
        <td rowspan="3">ru</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.8.0/25</td>
        <td>msk</td>
    </tr>
    <tr>
        <td>172.16.8.128/25</td>
        <td>kzn</td>
    </tr>
    <tr>
        <td>172.16.10.0/23</td>
        <td rowspan="3">sp</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.10.0/25</td>
        <td>bcn</td>
    </tr>
    <tr>
        <td>172.16.10.128/25</td>
        <td>mlg</td>
    </tr>
    <tr>
        <td>172.16.12.0/23</td>
        <td rowspan="3">cn</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>172.16.12.0/25</td>
        <td>sha</td>
    </tr>
    <tr>
        <td>172.16.12.128/25</td>
        <td>sia</td>
    </tr>
</table>

Underlay:
<table border="2">
    <tr>
        <td><b>Префикс</b></td>
        <td><b>Регион</b></td>
        <td><b>ДЦ</b></td>
    </tr>
    <tr>
        <td>10.0.0.0/17</td>
        <td rowspan="3">ru</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>10.0.0.0/19</td>
        <td>msk</td>
    </tr>
    <tr>
        <td>10.0.32.0/19</td>
        <td>kzn</td>
    </tr>
    <tr>
        <td>10.0.128.0/17</td>
        <td rowspan="3">sp</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>10.0.128.0/19</td>
        <td>bcn</td>
    </tr>
    <tr>
        <td>10.0.160.0/19</td>
        <td>mlg</td>
    </tr>
    <tr>
        <td>10.1.0.0/17</td>
        <td rowspan="3">cn</td>
        <td>&nbsp;</td>
    </tr>
    <tr>
        <td>10.1.0.0/19</td>
        <td>sha</td>
    </tr>
    <tr>
        <td>10.1.32.0/19</td>
        <td>sia</td>
    </tr>

</table>


<hr>


<a name="REALITY"></a>
<h1>Лаба</h1>


Два вендора. Одна сеть. АДСМ.

Juniper + Arista. Ubuntu. Старая добрая Ева.

Количество ресурсов на нашей виртуалочке в Миране всё же ограничено, поэтому для практики мы будем использовать вот такую упрощённую до предела сеть. 

<img src="https://fs.linkmeup.ru/images/adsm/2/lab.png" width="800">

Два датацентра: Казань и Барселона.
<ul>
    <li>По два спайна в каждом: Juniper и Arista.</li>
    <li>По одному ТоРу (Leaf'у) в каждом, с одним подключенным хостом (возьмём легковесный Cisco IOL для этого).</li>
    <li>По одной ноде Edge-Leaf (пока только Juniper).</li>
    <li>One Cisco switch to rule them all.</li>
    <li>Помимо сетевых коробок запущена виртуальная машина-управляка.
    Она имеет доступ ко всем устройствам, на ней будут крутиться IPAM/DCIM-системы, букет питоновских скриптов, анзибль и что угодно ещё, что нам может понадобиться.</li>
</ul>

Полная конфигурация всех сетевых устройств, которую мы будем пробовать воспроизвести с помощью автоматики.!!!!
<hr>

<a name="THEEND"></a>
<h1>Заключение</h1>
Так же принято? Под каждой статьёй делать короткий вывод? 
Итак мы выбрали трёхуровневую сеть Клоза внутри ДЦ, поскольку ожидаем много East-West трафика и хотим ECMP.
Выбрали BGP в качестве протокола маршрутизации анедрелейных сетей за его масштабируемость и гибкость политик.
У нас будут отдельные узлы для организации DCI - Edge-leaf.
На магистрали будет OSPF+LDP.
DCI будет реализован на основе MPLS L3VPN.
Для P2P-линков IP-адреса мы будем вычислять алгоритмически на основе  имён устройств.
Лупбэки будем назначать по роли устройств и их расположению последовательно.
Андерлейные префиксы - только на Leaf-коммутаторы последовательно на основе их расположения.

Предположим, что прямо сейчас у нас ещё не установлено оборудование.
Поэтому наши следующие шаги будут - завести их в системах (IPAM, инвентарная), организовать доступ, сгенерировать конфигурацию и задеплоить её. 

В следующей статье разберёмся с Netbox - системой инвентаризации и управления IP-пространством в ДЦ.


<a name="LINKS"></a>
<h1>Полезные ссылки</h1>
<ul>
    <li>Подход к маршрутизации в L3 ДЦ описан в следующих документах:
    - <a href="https://tools.ietf.org/html/rfc7938">RFC 7938. Use of BGP for Routing in Large-Scale Data Centers</a>
    - <a href="https://cumulusnetworks.com/lp/bgp-ebook/">Cumulus. BGP in the data center</a></li>
    <li>Короткая вводная в сеть Клоза: <a href="https://www.networkworld.com/article/2226122/clos-networks-what-s-old-is-new-again.html">Clos Networks: What's Old Is New Again</a></li>
    <li>Про сеть Клоза до мельчайших винтиков:
    Demystifying DCN Topologies: Clos/Fat Trees – <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/">Part1</a> и <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part2/">Part2</a></li>
</ul>
