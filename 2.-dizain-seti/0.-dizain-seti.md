Описанные в этой серии практики применимы к сети любого типа, любого масштаба с любым многообразием вендоров (нет).

Однако нельзя описать универсальный пример применения этих подходов. Поэтому я остановлюсь на современной архитектуре сети ДЦ: Фабрике Клоза.
В этом случае получится сравнительно простой сценарий для автоматизации, потому что имеем много оборудования, настраивающегося одинаковым образом. 
Мы выберем сферический ДЦ в вакууме:
    - Одна версия дизайна везде. 
    - Два вендора, образующих два плейна сети.
    - Один ДЦ похож на другой как две капли воды.

Но ближе к практике.

Пусть наш Сервис-Провайдер будет, например, хостить обучающие видео о выживании в застрявших лифтах.
В мегаполисах это пользуется бешенной популярностью, поэтому физических машин надо много.


Локации
У linkmeup будет 6 ДЦ: 
Россия (RU):
    - Москва
    - Казань
Испания (SP):
    - Барселона
    - Малага
Китай (CN):
    - Шанхай
    - Сиань

В каждом ДЦ по 100 стоек
В каждой стойке по 30 машин.
Также в каждой стойке стоит коммутатор, к которому подключены все машины - это модель Top of the Rack switch (в противовес EoR - End of Row).

Каждый ToR-коммутатор в свою очередь соединён с вышестоящим агрегирующим коммутатором - Spine. Под Spine'ы выделено по 2 стойки из тех 100
И ещё две - под сетевое оборудование для связности между ДЦ. 
Между собой ДЦ связаны арендованными лямбдами и WDM оборудованием, что впрочем, в контексте задачи значения не имеет.
В Москве организован выход во внешний мир.

Вот такая верхнеуровневая схема получается.
Внутри все ДЦ похожи между собой, как две капли воды: Двухуровневые Фабрики Клоза.
Каждый Leaf (ToR) соединён с каждый Spine'ом.
Каждый Spine соединён с каждым Leaf'ом. 

Между Leaf и машинами - 10 Гб/с.
Между Spine и Leaf - 400 Гб/с, что означает, что каждый Leaf имеет по одному линку с четырьмя Spine'ами.

Картинка!!!

Задача сетевой фабрики при этом очень и очень простая - обеспечить IP-связность между машинами как в пределах одного ДЦ, так и между. 
Оттого-то сеть и называется фабрикой по аналогии с фабриками коммутации внутри модульных сетевых коробок, о чём подробнее можно почитать в конце статьи. 

Фабрика полностью L3. Никаких VLAN, никаких Broadcast - вот такие у нас в linkmeup замечательные программисты, умеют писать приложения, живущие в парадигме L3, а виртуальные машины не требует Live Migration c сохранением IP-адреса.

Для маршрутизации будем использовать BGP (следует отметить, как в середине СДСМ мы наивно смеялись над идеей BGP в качестве IGP, а теперь к ней обратились).

Далее обо всём этом поподробнее









Физическая топология.

То, с чем мы будем далее работать, называется Фабрикой Клоза.
Её основная идея в том, то есть входы, есть выходы, а между ними коммутирующая матрица. В случае сети входы - они же выходы - это интерфейсы ToR-коммутаторов.
!!!! Картинка

И задача сети обеспечить возможность одновременной коммуникации любого количества пар вход-выход на полной скорости.

Почему это так важно, почему нельзя использовать классическую модель?

Тут никуда без истории.

Fat Tree
Традиционные сети ДЦ строились по модели Fat Tree: чем дальше от доступа, тем толще линки.

Картинка.
100 Мб/с к машинам, 1 Гб/с от коммутаторов доступа к коммутаторам агрегации. 
10 Гб/с (а в то время LAG из нескольких гигабитных линков) к ядру сети.
Итд.

На верху иерархии стояла пара God Box, концентрирующих на себе все сервисы и всю маршрутизацию, на которые инженеры молились и старались не дышать. Вывести их из эксплуатации, для обновления например, означало потерю половины пропускной способности сети, а то и вовсе полную деградацию сервиса из-за того что второе устройство настроено неправильно. 

Расширение пропускной способности - это отдельная болезненная история:
Расширение LAG'ов,
Закупка дорогущих линейных карт для God Box'ов.
А если карты ставить больше некуда, то апгрейд железа, на ещё более мощные и большие ящики.
Это очень удачный момент для того, чтобы обновить не только железо, но и архитектуру.

Fat Tree не может обеспечить лёгкое расширение пропускной способности.
Он совершенно не масштабируется: добавление нового модуля в ДЦ потребует снова расширения линков.
Он не может обеспечить высокую степень резервирования - в классической L2-топологии использование избыточных линков затруднительно из-за "особенностей" MSTP. Соответственно часть линков просто простаивает.  А при выпадении узла есть риск потерять не только пропускную способность, но и всю сеть на некоторое время.
Эксплуатировать и поддерживать Fat Tree проблематично.

Любопытно, что L2-топология, обычно используемая в Fat Tree, настолько избаловали разработчиков приложений, то они уже не представляют своей жизни без L2. 

Здесь и кроются истоки TRILL, SPB, QFabric, EVPN и прочих технологий, так желающих облегчить жить Ops'ам.

Однако всё это сущие мелочи, которые могли бы решиться. Проблема в том, что 

Исторически запрос пользователя полностью обрабатывался в рамках одного хоста. Был запрос от клиента, который пришёл сверху, и ответ, уходящий обратно.
В такой сети преобладает вертикальное (South-North) направление. И Fat Tree такой спрос вполне удовлетворял. 

Но в современных ДЦ преобладает горизонтальный (East-West) трафик между машинами. 
И вот как это получается:
клиент запросил веб-страничку, 
Запрос пришёл на фронт-енд, 
тот послал запрос на один бэк-енд сервер, чтобы получить текстовый контент.
На другой сервер, чтобы получить картинки
На третий - отправил метаданные запроса - куки, версию ОС, браузера, регион 
Этот третий для анализа, пополнения статистики и формирования таргетированного контента отправил запрос на четвёртый.
А четвёртый не только сформировал адресную рекламу, но и дополнил коллектор Big Data на пятом севереа новыми данными, чтобы ещё лучше знать какие отмычки для лифтовых дверей предпочитает среднестатистический житель Ленинского района города Новосибирска, пользующийся Android 5.4 !!!
Фронт-енд скомпилировал страничку и отдал её пользователю
То есть б!ольшая часть событий произошла внутри сети сервис-провайдера.



Clos Fabric
Современный подход в строительстве ДЦ, которые используют мега- и гипер-скейлы, вроде Яндекса, фейсбука и амазона: разивитие идей Чарльза Клоза.

Картинка!!!
 


Он в !!!! Году предложил неблокируемую сеть!!!, в которое любое количество 
Входов могли одновременно общаться с нужными им выходами.

В 90-е годы эта идея обрела вторую жизнь с развитием пакетной коммутации.
В модульном сетевом оборудовании стало уже сложно соединять каждый порт с каждым, поэтому появился дополнительный уровень, в общем сокрытый от пользователей, - фабрики коммутации, задачей которых было доставить пакеты от входного порта в выходной, убрав при этом необходимость в полносвязной топологии.

Ну и последняя на сегодняшний день инкарнация - это датацентровые сети, которые суть - фабрики коммутации.
Вместо достаточно интеллектуального, а соответственно и склонного к ошибкам и багам уровня агрегации появляется уровень коммутации - Spine, задача которого - просто очень быстро переложить пакет с одного Leaf на другой.

В таком сценарии интеллект отчасти перемещается на Leaf'ы, отчасти на выделенные сетевые узлы, обеспечивающие связь с другими ДЦ и внешним миром. 

<blockquote>
Я тут достаточно фривольно обращаюсь терминами Leaf и ToR, как синонимами. Однако это не так.
ToR - это конкретная роль устройства в физической топологии.
Leaf - это конечное устройство в терминах Фабрики Клоза.
Так Leaf'ом может быть EoR - коммутатор, например.
</blockquote>

В чём плюсы такого схемы?
Во-первых, в отсутствии неиспользуемых линков (при учёте, что мы отказываемся от L2)
Во-вторых, конечно в широких горизонтальных каналах.
В-третьих, выпадение одного устройства не влечёт фатальных последствий:
Если это был ToR - то пострадает только одна стойка.
Если Spine - просядет пропускная способность, но не на 50%, как это было бы в Fat Tree, а лишь на 1/n, где n - число спайнов. 
В-четвёртых, простота вывода спайнов из эксплуатации. Благодаря небольшой деградации и отсутствию интеллекта на этом узле, проводить работы на них не так страшно, как на God-Box-ах
В-пятых, масштабируемость. Для увеличения пропускной при добавлении нового Leaf'а просто организуется новый линк, не нужно расширять Lag'и и вставлять новые платы.


На рисунке выше была представлена двухуровневая !!! Фабрика.

Но если нужно подключить ещё больше Leaf'ов, то можно надстроить ещё один уровень Spine'ов:
!!!
У гиперскейлов количество уровней на фабрике доходит до 5.

Отлично. А что делать с адресацией и маршрутизацией?

IP-план

Прежде чем нырнуть в планирование IP-пространства, стоит сказать пару слов о том, как работают сервисы в ДЦ.

В своём примере мы будем следовать идее, что фабрика максимально простая и обеспечивает только базовую IP-связность. ToR'ы выполняют роль PE-устройств, и они ничего не знают о сервисах - такой информацией обладают виртуальные маршрутизаторы (vRouter) или виртуальные коммутаторы (vSwitch), запущенные на гипервизорах.

Давайте сразу мыслить в терминах IPv6?
Сеть 10/8 продана под интерконнекты ToR-физическая машина
!!!
Для интерконнекта Leaf-Spine будем использовать FE80::/!!, а конкретно FE80::1/64 на Spine и FE80:FF/64 на Leaf. Везде одинаковые. 
Интерконнект между сервисным ToR'ом и бордером: !!!!

На каждом устройстве будет VLAN 10 для подключения физических машин. IP-адрес на VLAN10 будет шлюзом по умолчанию для них.


Маршрутизация
Подход к маршрутизации в L3 ДЦ описан в следующих документах:
Лапухов
Меланокс.!!!

Что интересно, для маршрутизации мы не используем IGP, ввиду отсутствия масштабируемости.
Возьмём в оборот EBGP, а конкретно BGP LU.
BGP Labeled Unicast заменяет собой связку IGP+LDP. Он присваивает маршруту метку и такой Labeled unicast route анонсирует соседу в Address Family !!!!
Картинка!!!

Выглядеть это будет так:
!!!! Картинка с распространением маршрута

Tor1 передаём маршрут до !!!! Своим соседям spine1 и spine2. В обоих случаях  в качестве nexthop будет FE80::FF и выделится метка 3 (Implicit Null).
Spine1 и Spine2 оба анонсируют этот маршрут Tor2
Анонс от Spine1: 
Метка: 1000
Nexthop: FE80::1

Анонс от Spine2:
Метка: 2000
Nexthop: FE80::1

На Tor2 придёт два маршрута с разными метками, но одним Nexthop. Фокус здесь в том, что разными будут выходные интерфейсы. И в случае Link Local IPv6, адрес неразрывно связан с интерфейсом, на котором эта сеть настроена.


!!!! Картинка с передачей пакета
Теперь пакет с машины2 приходит на Tor2. Он адресован в сеть !!!
Tor2 из FIB знает, что нужно инкапсулировать пакет в MPLS с меткой 1000 (или 2000 - работает балансировка) и отправить в интерфейс1 (2). DMAC выясняется из таблицы соседств (Adjacencies table) на основе пары Nexthop IP-интерфейс.

Пакет попадает на Spine1 (Spine2).
Тот видит, что метку 1000 (2000) нужно поменять на 3 (то есть вообще убрать) и отправить в интерфейс1

Tor1 обрабатывает его уже как обычный IP-пакет.


!!!
Для практики мы будем использовать вот такую сеть. 

Управлятор - это специальный хост для управления устройствами и запуска различных сервисов.
Вместе со всем сетевым оборудованием он подключен в коммутатор управления - Out of band Management.
На этом хосте будут крутиться системы инвентаризацией и управления IP-пространством, python с букетом скриптов и ansible.
