ECMP - это всё

Описывая дизайн сети датацентра, сложно обойти стороной эволюцию подходов, которая объясняет, почему большинство контент-провайдеров, строящих свои датацентры сегодня выбирают именно Фабрику Клоза.

Сеть в большой степени следует за требованиями сервисов, лишь иногда диктуя, как было бы правильно сделать.

В прошлом датацентры жили примерно в таких условиях:
    • Преобладающий North-South трафик от клиента к серверу и обратно.
    • L2 сеть в силу требования мобильности виртуальных машин без смены IP-адреса и высокой цены на L3-оборудование.
    • Любое сетевое оборудование, обладающее широкой функциональностью, стоило действительно немалых денег.

Поэтому традиционная сеть ДЦ была такой:


<h1>Fat Tree</h1>

<img src="https://fs.linkmeup.ru//images/adsm/2/fat_tree.png" width="600">

Чем дальше от доступа, тем толще линки.

100 Мб/с к машинам, 1 Гб/с от коммутаторов доступа к коммутаторам агрегации. 
10 Гб/с (а в то время, возможно, LAG из нескольких гигабитных линков) к ядру сети.
Итд.
Потому что весь трафик от серверов сливался в аплинки.

Наверху иерархии стояла пара <a href="https://comeroutewithme.com/2015/02/28/why-is-clos-spineleaf-the-thing-to-do-in-the-data-center/">God Boxes</a>, концентрирующих на себе все сервисы и всю маршрутизацию, на которые инженеры молились и старались не дышать. 
Это были модульные шкафы, занимающие полстойки, а то и стойку, а то и кластера из нескольких стоек. 
Вывести их из эксплуатации, для обновления например, означало потерю половины пропускной способности сети, а то и вовсе полную деградацию сервиса из-за того что второе устройство настроено неправильно. 

Расширение пропускной способности - это отдельная болезненная история:
    • Расширение LAG'ов,
    • Закупка линейных карт.
    • А если карты ставить больше некуда, то апгрейд железа, на ещё более мощные и большие ящики.

<blockquote>
    <i>Не может быть более удачного момента для того, чтобы обновить не только железо, но и архитектуру.</i>
</blockquote>

Fat Tree не может обеспечить лёгкое расширение пропускной способности.
Он довольно плохо масштабируется: добавление нового модуля в ДЦ потребует снова расширения линков.
Он не может обеспечить высокую степень резервирования - в классической L2-топологии использование избыточных линков затруднительно из-за "особенностей" MSTP. Соответственно часть линков просто простаивает.  А при выпадении узла есть риск потерять не только пропускную способность, но и всю сеть на некоторое время.
Эксплуатировать и поддерживать Fat Tree проблематично.

Любопытно, что L2-топология, обычно используемая в Fat Tree, настолько избаловала разработчиков приложений и системных администраторов, что они долгое время не представляли своей жизни без L2. 

Здесь и кроются истоки TRILL, SPB, FabricPath, EVPN и прочих технологий, так желающих облегчить жить Ops'ам.

<hr>

<h1>Северо-Юг против Запада-Востока</h1>
Однако всё это в некотором смысле мелочи, которые можно было бы решить.
Проблема в том, что изменился мир.
Внезапно появились Big Data, Map Reduce, ML, гигантские базы данных, аналитика, контекстная реклама. А теперь и AI из пауэр-поинтов на нас поглядывает. 

Исторически запрос пользователя полностью обрабатывался в рамках одного хоста. Был запрос от клиента, который пришёл сверху, и ответ, уходящий обратно наверх.
В такой сети преобладает вертикальное (North-South) направление. И Fat Tree такой спрос вполне удовлетворял. 
<img src="https://fs.linkmeup.ru//images/adsm/2/north_south.png" width="600">

Теперь в ДЦ преобладает горизонтальный (East-West) трафик между серверами. 
И вот как это получается:
<ol>
    <li>Клиент запросил веб-страничку,</li>
    <li>Его запрос пришёл на балансировщик трафика,</li>
    <li>Балансировщик перенаправил его на один из фронт-ендов, </li>
    <li>Тот послал запрос на один бэк-енд сервер, чтобы получить текстовый контент,</li>
    <li>На другой сервер, чтобы получить данные о пользователе: пол, возраст, предпочтения, последний поиск,</li>
    <li>На третий - проанализировать данные по локации,</li>
    <li>Второй и третий послали новые данные на четвёртый, чтобы пополнить информацию о пользователе в БД, чтобы ещё лучше знать какие отмычки для лифтовых дверей предпочитает среднестатистический житель Ленинского района города Новосибирска, пользующийся Android 9.1,</li>
    <li>На пятый - сформировать контекстную рекламу,</li>
    <li>Потом все разом начали отвечать фронтенду,</li>
    <li>Фронт-енд отдал всю страницу обратно</li>
</ol>

<img src="https://fs.linkmeup.ru//images/adsm/2/west_east.png" width="600">


То есть б!ольшая часть событий произошла внутри сети сервис-провайдера.
При этом запрос-ответ в ДЦ может многократно превышать по размеру запрос-ответ пользователю. Добавим к этому репликации БД, внутренние хранилища, логи и всё становится понятно. 

Fat Tree органически не приспособлен к этому из-за своего акцента на North-South.
L2-топологиям из-за низкой утилизации полосы и склонности к штормам тоже отказано.
Ну и умное железо если и не стало резко дешевле, то только из-за всё возрастающей производительности, доходящей сегодня уже до 12,8 Тб/с на одночиповую коробку размером 4 юнита.
А так уже любая вафельница умеет в L3+BGP.


<h1>Clos Fabric</h1>
Современный подход в строительстве ДЦ, которые используют гипер-скейлеры и клауд-титаны, вроде Яндекса, Фейсбука и Гугла - развитие идей разработчика из Bell Laboratories Чарльза Клоза.

Картинка!!!


Он в 1953 году, решая проблему проключения звонков через телефонную станцию, предложил неблокируемую сеть, в которой любые входы могут установить соединение с любыми выходами в любой момент времени.

Прежде для решения этой задачи использовался так называемый Crossbar, соединявший каждый вход с каждым выходом.
<img src="https://fs.linkmeup.ru//images/adsm/2/cross_bar.png" width="800">
<i>Изображение с сайта <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/" target="_blank">packetpushers.net</a></i>

Учитывая, что большая их часть простаивала, решение и без того сложное в реализации было ещё и не оптимальным.

Он предложил разделить входы и выходы дополнительным уровнем коммутации, который позволит простраивать соединения между входами и выходами по запросу.
<img src="https://fs.linkmeup.ru//images/adsm/2/clos_network_1.png" width="600">

Сложность такой сети значительно ниже, чем у Crossbar.



В чистом виде она описывала сети с коммутацией каналов.
Но в 90-е годы эта идея обрела вторую жизнь с развитием пакетной коммутации.
В модульном сетевом оборудовании стало уже сложно соединять все SerDes'ы одного чипа со всеми SerDes'ами другого, поэтому появился дополнительный уровень, в общем скрытый от пользователей, - фабрики коммутации, единственной задачей которых было доставить пакеты от входного порта в выходной, убрав при этом необходимость в полносвязной топологии.

Ну и последняя на сегодняшний день инкарнация - это датацентровые сети, которые суть - фабрики коммутации.

<img src="https://fs.linkmeup.ru//images/adsm/2/clos_network_2.png" width="600">
<i>Типичный вид сети Клоза</i>

<img src="https://fs.linkmeup.ru//images/adsm/2/clos_network_3.png" width="600">
<i>Более привычный вид Leaf-Spine</i>

Вместо весьма интеллектуального, а соответственно и склонного к ошибкам и багам уровня агрегации появляется примитивный уровень коммутации - Spine, задача которого - аналогично очень быстро переложить пакет с одного Leaf на другой.

Выход же во внешний мир или в другие ДЦ обычно реализуется через отдельные коробки, которые с точки зрения фабрики выглядят как Leaf-коммутаторы, однако гораздо более функциональные. Называются они Edge-Leaf. 

<img src="https://fs.linkmeup.ru//images/adsm/2/clos_network_4.png" width="600">

В таком сценарии интеллект отчасти перемещается на Leaf'ы, отчасти на Edge-Leaf'ы, обеспечивающие связь с другими ДЦ и внешним миром. 


В чём плюсы такой схемы?
Во-первых, в отсутствии неиспользуемых линков (при учёте, что мы отказываемся от L2)
Во-вторых, конечно, в широких горизонтальных каналах.
В-третьих, выпадение одного устройства не влечёт фатальных последствий:
Если это был ToR - то пострадает только одна стойка.
Если Spine - просядет пропускная способность, но не на 50%, как это было бы в Fat Tree, а лишь на 1/n, где n - число спайнов. 
В-четвёртых, простота вывода спайнов из эксплуатации. Благодаря небольшой деградации и отсутствию интеллекта на этом узле, проводить работы на них не так страшно, как на God-Box'ах
В-пятых, масштабируемость. Для увеличения пропускной способности при добавлении нового Leaf'а просто организуется новый линк, который сразу попадает в ECMP-группу.
Не нужно расширять LAG'и и вставлять новые платы. А если вдруг портов уже не хватает или хочется объединять модули одного ДЦ в суперфабрику, то можно надстроить ещё один уровень спайнов (или не один).
<blockquote>
    Такой метод масштабирования называется <b>горизонтальным</b> или <b>Scale out</b>. Добавляется просто ещё одно типовое устройство, практически линейно увеличивая пропускную способность. Нынче это очень модно. Теперь всё масштабируют горизонтально.
    Классический подход - <b>вертикальное</b> масштабирование <b>Scale Up</b> - тут добавляют мощностей в текущие устройства, чтобы расширить их возможности. В маршрутизаторы добавляют платы, в сервера - оперативную память и процы.
</blockquote>

Здесь может сложиться впечатление, что мы просто перенесли всю сложность с God-Box'ов на граничные маршрутизаторы. Что же, впечатление не обманывает: так оно и есть. Но L3 здесь позволяет, используя ECMP, масштабировать внешнюю связность, просто увеличивая количество граничных коробок. И также легко и безопасно выводить их из работы, как спайны (почти). 

<hr>


<a name="THEEND"></a>
<h1>Заключение</h1>

Картинка, как по мне, красивая. Но всегда следует помнить, что и инженеры, и админы, и девопсы, и архитекторы решают задачи бизнеса, а не диктуют как бизнесу подстраиваться под классную сеть, которую они придумали. Поэтому  место найдётся и для clos-топологий и для Fat-Tree и для MSTP в ядре (увы).
Кстати, сеть Клоза на самом деле изобретена не Клозом, а Эдсоном Эрвином на 15 лет раньше, но кого это теперь волнует? :)


<a name="LINKS"></a>
<h1>Полезные ссылки</h1>

<ul>
    <li><a href="https://www.gdt.id.au/~gdt/presentations/2016-07-05-questnet-sdn/papers/bell195303--clos--a-study-of-non-blocking-switching-networks.pdf">Оригинальная публикация <i>A study of non-blocking switching networks by Charles Clos</i></a>.
    Натурально научный угар. Если не верите, вот вам скринпруф:
    <img src="https://fs.linkmeup.ru//images/adsm/2/formulas.png" width="500">
    </li>
    <li>Короткая вводная в сеть Клоза: <a href="https://www.networkworld.com/article/2226122/clos-networks-what-s-old-is-new-again.html">Clos Networks: What's Old Is New Again</a></li>
    <li>Про сеть Клоза до мельчайших винтиков:
    Demystifying DCN Topologies: Clos/Fat Trees – <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part1/">Part1</a> и <a href="https://packetpushers.net/demystifying-dcn-topologies-clos-fat-trees-part2/">Part2</a></li>
    <li>О вреде God Boxes в ДЦ: <a href="https://comeroutewithme.com/2015/02/28/why-is-clos-spineleaf-the-thing-to-do-in-the-data-center/">Why is Clos (Spine/Leaf) the thing to do in the Data Center?</a>.</li>
</ul>

