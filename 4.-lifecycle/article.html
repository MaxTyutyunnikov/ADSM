Продолжаем наш <a href="https://linkmeup.ru/adsm/">забег по сетевой автоматизации</a>.
Итак, сеть спроектирована, IPAM запущен. И вот-вот начнут съезжаться миллионы наших стоек. Будем готовиться к этому.
Мы всё дальше от фантазий и абстрактных разговоров и ближе к практике.
И всё же снова сделаем отступление. Большое дело начинается с большого перекура.

Сеть полезно представлять, как некое единое целое, которое мы переводим из одного состояния в другое. Сервис мы внедряем на всей сети. Не может быть такого, что он работает только на 3 устройствах из 4 необходимых. <b>Вся</b> сеть должна обеспечивать отказоустойчивость и достаточную полосу.
Однако рано или поздно всё равно любая задача декомпозируется до уровня отдельных сетевых коробок.
И если про сеть как единый организм мы уже поговорили  в <a href="https://linkmeup.ru/blog/424.html">0-й статье</a>, то теперь пришло время разобраться отдельными органами.

<img src="https://fs.linkmeup.ru/images/adsm/4/kdpv.png" width="800">

В этой статье разберём жизненный цикл сетевого устройства и некоторые сценарии того, какие манипуляции с ним приходится порой делать. 
Естественно, всё это интересует нас с точки зрения автоматизируемости. Поэтому ещё мы нарисуем архитектуру системы автоматизации. 
Кстати, не так давно вышла просто восхихитительная <a href="https://dteslya.engineer/network_automaiton_101/">обзорная статья</a> Дмитрия Тесля о процессе и инструментах сетевой автоматизации. Он смог лаконично изложить то, вокруг чего я пляшу уже несколько выпусков АДСМ. Настоятельно рекомендую прочитать её перед тем, как преступать к этой.

<cut>

<h1>Содержание</h1>
<ul>
    <li>
        <b><a href="#LIFECYCLE">Жизненный цикл оборудования</a></b>
        <ul>
            <li><a href="#DAY0">DAY 0</a></li>
            <li><a href="#DAY1">DAY 1</a></li>
            <li><a href="#DAYN">DAY N</a></li>
        </ul>
    </li>
    <li><b><a href="#IAC">IaC</a></b></li>
    <li>
        <b><a href="#BIRD_VIEW">Система автоматизации</a></b>
        <ul>
            <li><a href="#PRINICIPLES">Характеристики системы</a></li>
            <li><a href="#SCENARIOS">Сценарии</a>
                <ul>
                    <li><a href="#CHECKS">0. Проверка сети</a></li>
                    <li><a href="#NEW">1. Ввод нового оборудования</a></li>
                    <li><a href="#SOT_TRIGGERED">2. Переконфигурация из-за изменений переменных в инвентарной системе</a></li>
                    <li><a href="#DESIGN_TRIGGERED">3. Переконфигурация из-за изменения дизайна</a></li>
                    <li><a href="#DATA_COLLECTING">4. Сбор информации с устройств</a></li>
                    <li><a href="#DRAIN">5. Снятие и возврат нагрузки</a></li>
                    <li><a href="#UPGRADE">6. Обновление ПО</a></li>
                    <li><a href="#DELETE">7. Удаление устройства</a></li>
                    <li><a href="#REPLACE">8. Замена устройства</a></li>
                    
                </ul>
            </li>
        </ul>
    </li>
    <li><b><a href="#LINKS">Полезные ссылки</a></b></li>
</ul>
<hr>

Так, ну а мы про
<a name="LIFECYCLE"></a>
<h1>Общий взгляд на жизненный цикл оборудования</h1>

<img src="https://fs.linkmeup.ru/images/adsm/4/life_cycle.png" width="800">

Вот мы купили сетевую железку. Что теперь? Проследим её жизнь с первого и до последнего дня.
<ol>
    <li>
        <b>Day 0</b> - железка только появилась в наших руках. Сейчас самое важное - базовая настройка:
        <ul>
            <li>Добавить IP-адрес управления и маршрут</li>
            <li>Включить SSH</li>
            <li>Создать пользователя с правами настройки</li>
        </ul>
        Иными словами задача Day0 конфигурации - организовать доступ на устройство.
    </li>
    <li>
        <b>Day 1</b> - Железка уже встала на позицию, определена его роль в сети и сервисы, которые она обслуживает. 
        Теперь нужно настроить уже целевую конфигурацию, с которой устройство встанет в сеть под нагрузку.
    </li>
    <li>
        <b>Day N</b> - Изменения конфигурации в процессе эксплуатации.
        Бывает мы добавляем новый сервис, пересматриваем дизайн или на худой конец нужно ACL поправить.
        Такие изменения нужно тоже уметь довозить до устройства.
    </li>
    <li>
        <b>Обслуживание</b> - Помимо нормальной работы есть периоды, когда устройство нужно аккуратно вывести из под нагрузки, чтобы, например, поменять в нём плату, провести обновление ПО.
    </li>
    <li>
        <b>Отслеживание изменений</b> - со всей сети следует собирать информацию о том, где и во сколько применялась новая конфигурация. Это позволит как скоррелировать жалобы клиентов с изменениями, так и знать, когда применялась новая конфигурация в обход процедуры.
    </li>
    <li>
        <b>Проверка соответствия эталонной конфигурации</b> - В течение всей жизни устройства нужно проверять, что его конфигурация не разошлась с целевой из-за сбоев в автоматике, обновлений ПО или прямого вмешательства чьих-то рук. 
    </li>
    <li>
        <b>Бэкапы</b> - Даже если мы в любой момент можем сгенерировать эталонную конфигурацию, чтобы применить её на устройство, бэкапы необходимы.
    </li>
    <li>
        <b>The Last Day</b> - снятие нагрузки, удаление из всех систем, ритуальное сжигание. Под сжиганием я понимаю безопасную затирку конфигурации, чтобы хэши паролей (или, упаси Лейбниц, клиртекст), префикс-листы и ACL'и не оказались достоянием общественности.
    </li>
</ol>

Я намеренно обхожу вниманием в этой статье вопрос мониторингов операционного состояния и реакции на них, поскольку её лейтмотив - это всё же конфигурация.

<hr>
Далее обсудим Day0 - DayN более детально.

<a name="DAY0"></a>
<h1>Day0</h1>
Итак, поставщик привёз на склад новый свитч. Его нужно установить, настроить, проверить, запустить трафик, добавить во все системы: инвентарные, мониторинги, бэкапы, скрипты автоматизации всякой рутины.

Задачи Day 0 можно грубо разделить на две части:

<ul>
    <li>Завести устройство в системах</li>
    <li>Настроить базовый доступ</li>
</ul>
Говорить про них в отрыве друг от друга сложно, и делать мы так не будем.

Какие же есть способы?
<ol>
    <li>
        <b>Бумер</b>  - вручную завести устройство в инвентарной системе и выделить свободный IP-адрес. Пусть это будет даже экселька.
        Подключить свитч к компьютеру и через консольный порт настроить IP-адрес, маршрут, включить SSH, создать пользователя.
        Отвезти свитч на позицию.
            <b>+</b> Просто, не требует почти никакой инфраструктуры.
            <b>-</b> Склонно к ошибками, масштабируется человеко-часами. 
    </li>
    <li>
        <b>Бумер+</b> - автоматизируем заведение устройства в DCIM/IPAM. Мы только нажимаем кнопочку, а в системе появляется железка на правильной локации со всеми нужными портами, ей выделяется автоматически имя и следующий свободный IP-адрес. В итоге генерируется базовый конфиг в виде текстового файлика.
        Администратор подключает свитч к компьютеру и через консольный порт копипастит содержимое этого файлика в терминал.
            <b>+</b> Ниже вероятность ошибок, значительно меньше ручной работы
            <b>-</b> Требуется уже какая-никакая инфраструктура: IPAM/DCIM с API, скрипт, всё ещё ручная работа, всё ещё настраивать на стенде и потом везти устройство на позицию.
    </li>
    <li>
        <b>Миллениал</b> -  ZTP - Zero Touch Provisioning - подход, которому 100 лет в обед, но он почему-то всё ещё есть не везде. Идея в том, что устройство сразу же ставится на позицию и подключается в сеть управления, после чего по DHCP оно само получает свою конфигурацию. 
        Для этого устройство должно быть уже заведено в IPAM/DCIM и предгенерирована конфигурация, которая и передаётся устройству. 
            <b>+</b> Устройство можно сразу везти на позицию, минимум ручного труда
            <b>-</b> Нужна уже продуманная связная инфраструктура: IPAM/DCIM, DHCP, (T)FTP, автогенерация конфигов. Классическую вендорскую реализацию сложно применить для распределённых сетей, вроде ритейла.
    </li>
    <li>
        <b>Зумеры</b> - SD-WAN. Кстати, как раз подходит для ритейлов, хотя в свою очередь не очень для датацентров. Подход разделяет идею ZTP - мы устройство включаем, а оно само настраивается.
            <b>+</b> Меньше вероятность ошибок. На первый взгляд меньше работы
            <b>-</b> Однако SD-WAN - это преимущественно проприетарные решения вендоров, требующие мощной инфраструктуры, причём иногда только в облаке вендора. У нас, кстати, был целый подкаст про SD-WAN: <a href="https://linkmeup.ru/blog/588.html">telecom №91. SD-WAN</a>.
    </li>
    <li>
        <b>Пост-хипстеры</b> - есть компании, где помимо Out of Band сети управления, есть ещё консольное соединение до абсолютно каждой железки. Для этого есть соответственно сеть консольных серверов внутри датацентров и точек присутствия.
        Каждое новое устройство после установки подключается в OOB-свитч по Ethernet и в консольный сервер консольным кабелем.
        Это позволяет реализовать схему, подобную описанной ниже:
        <ul>
            <li>Устройство добавляется в IPAM/DCIM</li>
            <li>Устройство устанавливается и подключается по управлению</li>
            <li>Инженер в ДЦ создаёт задачу на сервер наливки: <i>настроить свитч, как Leaf, за консольными сервером №7, порт 3</i></li>
            <li>Сервер наливки подключается на указанный порт, забирает серийный номер, с которым идёт в IPAM, генерирует базовый конфиг и обратно через тот же консольный порт применяет данную конфигурацию</li>
        </ul>
            <b>+</b> Всегда есть консольный доступ на устройство, какие бы шторма ни гуляли в сети трафика и управления. Нет проблем с вендорскими особенностями - консольный протокол у всех реализован одинаково (с поправкой на параметры порта)
            <b>-</b> Совсем непросто и в абсолютных цифрах недёшево реализовывать ещё одну сеть управления. Не подходит для географически распределённых сетей. Требуется серьёзная инфраструктура даже в минимальном варианте без использования сервера наливки.
    </li>
</ol>

Как видите, любые решения по автоматизации Day 0 требуют чего-то больше, чем просто скриптик на питоне. К этому процессу нужно подходить системно с точки зрения выстраивания инфраструктуры.
Кстати, вот классный доклад от фейсбука про их Вендинговые Машины по выдаче новых локаций: 

<video>https://www.youtube.com/watch?v=ErmhE_wmNo0</video>

Так или иначе эта часть автоматизирована у многих, потому что подходы понятны, инструменты в ассортименте.
<hr>

<a name="DAY1"></a>
<h1>Day 1</h1>
Дальше на железку нужно накатить уже рабочую конфигурацию и пустить на неё нагрузку.
Тут уже заметно интереснее. Одно дело - сгенерировать простейший конфиг на 20 строчек, одинаковый для всех типов устройств, как было в Day 0, и совсем другое - целевой конфиг на пару тысяч строк, который может радикально отличаться от железки к железке в зависимости от её роли и необходимых сервисов. Например, конфигурации двух экземпляров одной и той же модели свитча, установленных в качестве лифа и спайна, будут различаться как минимум настройками даунлинк интерфейсов. 
Основная идея здесь в том, что мы описываем дизайн сети в том или ином формальном виде и отдаём его генераторам. Генераторы берут этот дизайн, роль устройства, локацию, переменные из IPAM/DCIM, всё это перемешивают, а на выходе получается специфический для данной коробки конфиг.
То есть основных компонента здесь три:
<ol>
    <li>Формализованный дизайн</li>
    <li>Заполненные данные в IPAM/DCIM</li>
    <li>Набор генераторов</li>
</ol>

Здесь подробно останавливаться не будем - формализации дизайна я посвящу отдельный (и скорее всего не один) выпуск.

Итак, имеем конфиг Day1. Осталось всего ничего - применить его на железку. 
И тут все средства хороши в разных комбинациях: консоль, SSH, netmiko, NETCONF, GNMI, REST API, SNMP (я сейчас не шучу - лично видел), FTP, SCP.
В целом на нерабочую пока железку применить конфиг действительно можно разными способами:
<ul>
    <li>Ручной копипаст из файлика в терминал</li>
    <li>Применение команд последовательно через SSH из кода, используя тот же netmiko </li>
    <li>Копирование файла на флэшку устройства, установка его в качестве конфигурационного и ребут железки</li>
    <li>А-ля config replace</li>
    <li>Пульнуть через NETCONF весь конфиг в XML</li>
    <li>gNMI</li>
</ul>

Об этом тоже ещё поговорим. 

С автоматизацией этой задачи большинство тоже справляются - один раз настроить железку без нагрузки - дело нехитрое. 


Замечу, что если есть процесс и инструменты Configuration Management и версионирования конфигурации, то Day1  - это лишь частный случай DayN.

<hr>

<a name="DAYN"></a>
<h1>Day N</h1>
И вот теперь - ежедневная эксплуатация и периодические реконфигурации.
А вот с этим дела обстоят туго чуть менее, чем у всех. Говоря это, я не шучу. Тут всё плохо.
Дело в том, что нагенерить конфигурацию - действительно несложно. Пусть это будет даже циклопический jinja-шаблон с циклами и каунтерами.

А вот применить этот конфиг на железку ещё и под продуктивной нагрузкой - цель для инженеров со стальными нервами.

Тут целый ком проблем, как очевидных, так и неявных.

<b>Во-первых</b>, интерфейс: CLI, NETCONF, GNMI, SCP/FTP.
Если CLI - то как быть с особенностями реализации каждого вендора? Режимы контекстов, интерактивные диалоги, порядок выполнения команд.
Если NETCONF или gNMI - то его не все вендоры поддерживают. А те, кто поддерживает, делают это сильно по-разному, и зачастую не в полной мере. А если в полной мере, то, конечно, же в своей схеме, а не в OpenConfig.
А если файлик подложить - то не все на лету умеют заменять, а значит с ребутом - только кому он нужен при добавлении BGP-пира?

<b>Во-вторых</b>, инструмент доставки: netmiko, ncclient, ansible (какой модуль), SaltStack?

<b>В-третьих</b>, как заливать вслепую? Отправляя полную конфигурацию, мы не знаем, как она изменит состояние устройства. Даже если мы видим дифф между файлами или в ветке в гите, это не говорит о том, какие команды фактически применятся на железке.

<b>В-четвёртых</b>, даже если мы видим будущие изменения (кандидат-конфиг на самом устройстве, к примеру), то это не говорит о том, что мы ничего не разломаем по своей неосмотрительности. Тут уже напрашивается сетевой CI/CD.

<b>В-пятых</b>, весь ворох вопросов мультивендорной взрослой сети: разный синтаксис, семантика даже между версиями софта, где-то есть коммиты, где-то нет, где-то можно увидеть кандидат, где-то нет.

Это область компромиссов. 

Но давайте будем честны сами с собой: восьми компаниям из десяти не нужен выстроенный процесс версионирования конфигурации, конвейер CI/CD, автоматическая выкатка, а возможно, и вообще весь этот ваш DevOps в сети. 
Скорее всего, вам действительно достаточно залить первичный конфиг, а дальше изменения накатывать всю жизнь элементарными плейбуками, составленными вручную. И для этого, включая мониторинги и внутренние инструменты, достаточно 2-5 человек, а не целый штат разработчиков.
И большинство компаний именно так и делает. 
Можно добавить GitLab, TeamCity, AWX, аппаратную лабораторию с набором специфических тестов (FIB, QoS). Это всё мощные улучшайзеры, которые сделают процесс выкатки новой конфигурации значительно безопаснее. Но они не переведут управление конфигурацией на принципиально новый уровень.

<img src="https://fs.linkmeup.ru/images/adsm/4/deploy.gif" width="300">

А мы ведь всё же хотим
<ul>
    <li>Полную автоматизацию</li>
    <li>Универсальное решение</li>
    <li>Минимизацию рутины</li>
    <li>Безопасные выкатки конфигурации</li>
    <li>Формализованный дизайн</li>
    <li>Версионирование</li>
    <li>Транзакционность, а если быть точнее, то соответствие требованиям <a href="#ACID">ACID</a></li>
</ul>


Поэтому давайте составим схему системы автоматизации, которая позволит нам решить все задачи.
Но прежде расширим понятие "Инфраструктура как код" на сетевую инфраструктуру.

<hr>

<a name="IAC"></a>
<h1>IaC</h1>
Если вы не слышали о <b>IaC - Infrastructure as Code</b>, у меня для вас плохие новости. Очень плохие.
Ладно, не напрягайтесь, сейчас всё расскажу.

Как было раньше: выдали вам пачку физических машин - на каждой из них вы запустили KVM/VMWare/WTF, подняли на них флот виртуалочек, настроили сеть, выкатили своё приложение. Нужна дев-среда? Давайте всё в той же последовательности теми же руками. И во второй раз может получиться <i>чуть-чуть</i> не то же самое.
Парадигма IaC предполагает, что конфигурация всей инфраструктуры описывается в текстовых файлах. Речь как про физические устройства, так и про виртуальные машины, контейнеры и прочее.
Далее эти текстовые файлы обрабатываются неким инструментарием, который настраивает инфраструктуру на основе этой информации. Это может быть Terraform, Ansible, SaltStack.
Как результат - вы всегда быстро и с минимальным участием человека получаете предсказуемый результат.

Например, вы декларативно записываете в yml-, txt-, tf-, wtf-файле, что на таких-то хостах нужно установить KVM, Open vSwitch, настроить IP-адреса и туннели. Далее поднять набор ВМ с убунтой, выдать им адреса, на них установить nginx, загрузить ваш сайт в указанный каталог и настроить nginx. И поставить всё это дело за свежезапущенный балансер.
И получить всё это становится возможным всего одним запуском terrform, если желаемое состояние описано в конфигурационном файле. 
Данные в самом файле могут быть в форматах yml, json, это может быть набор объектов вашего любимого ЯП или что угодно иное, что может быть принято инструментом.
Строго говоря, даже если вы в bash-скрипт напихаете весь набор операций - это уже будет IaC. Просто пользоваться этим не очень удобно.

На схеме ниже изображена упрощённая процедура того, как изменения появляются на сети в парадигме IaC.
<img src="https://dteslya.engineer/images/2020-10-netdevops-pipeline.png">
<i>https://dteslya.engineer/network_automaiton_101/</i>

То есть настройка инфраструктуры выглядит аналогично релизу новой версии приложения в DevOps - те же гиты, апрувы, CI/CD и прочая. 

Что же до сетей?
Прежде в хорошей ситуации у нас для них был HLD (High Level Design), который руками превращался в LLD (Low Level Design) для каждой железки, готовился конфиг (скорее всего, руками) и заливался на железо (надо полагать, тоже руками). В плохой - инженер сразу на железке настраивал сервисы так, как ему казалось правильным. Я знаю - сам прошёл все эти стадии.

<s>Я же предлагаю вам качественно новую жизнь!</s>
Систему автоматизации сети мы будем рассматривать также в разрезе IaC. Давайте уже прекратим строить из себя особенных, потому лишь что у наших хостов проприетарная ОС. Сеть - такая же часть инфраструктуры, как физические машины, виртуалки, контейнеры. Ну да, прихлопнуть свитч с нерабочим чипом и пересоздать новый нельзя - но это просто добавляет красок в нашу работу.

<img src="https://fs.linkmeup.ru/images/adsm/4/its_different.jpg" width="600">

Обновление конфигурации на сети и обновление прочей инфраструктуры после этого - тот же процесс деплоя.

В общем это именно то, о чём мы тут толкуем с самого 0-го выпуска. Дизайн описан в формализованном HLD, а конкретные данные берутся из нашего SoT - Netbox. Из них генерится конфигурация и складывается в репозиторий, где прогоняются авто-тесты (в аппаратной или виртуальной лабе или что-то а-ля Batfish), кто-то смотрит глазами и подтверждает изменения, далее они по всем правилам CD выезжают в прод.

IaC - и ничегошеньки не настраиваем руками.

<img src="https://fs.linkmeup.ru/images/adsm/4/tobusy.png" width="500">

<hr>

<a name="BIRD_VIEW"></a>
<h1>Система автоматизации с высоты птичьего полёта</h1>
Один из принципов, который нужно заложить в систему - это Zero Touch Prod, то есть свести к минимуму прямое хождение инженера на устройства. Любые изменения конфигурации только через платформу, только через  интерфейс.



Итого, какой список задач решаем?
<ol>
    <li>Нужен интерфейс, через который можно создавать задачи. Он абстрагирует работу с сетевыми устройствами. 
    Графический - для инженеров, API - для внешних сервисов.</li>
    <li>Ввод новых устройств</li>
    <li>Актуализация данных в инвентарной системе (LLDP, список интерфейсов, IP-адресов, версия ПО)</li>
    <li>Генерация целевых конфигураций</li>
    <li>Применение целевых конфигураций и временных патчей (тшут, костыль)</li>
    <li>Сличение целевых и реальных конфигов</li>
    <li>Снятие и возврат нагрузки</li>
    <li>Обновление ПО</li>
    <li>Сбор бэкапов, коммитов, </li>
    <li>Диспетчеризация задач, выполняющихся на железе.</li>
</ol>

Соответственно схематично я бы изобразил это так:

<img src="https://fs.linkmeup.ru/images/adsm/4/bird_scheme.svg" width="800">

<ol>
    <li>
        <b><u>IPAM/DCIM</u></b> - система, являющаяся Source of Truth для всей системы автоматизации. В нашем случае - Netbox.
    </li>
    <li>
        <b><u>NetAPI</u></b> - служба одного окна. Что бы ни вздумалось сделать с сетью - идём в него. 
        Например, захотелось добавить новый свитч - идём в ручку NetAPI с нужным набором параметров (серийник, имя, локация) и создаём задачу на добавление свитча. А-ля: https://netapi.linkmeup.ru/api/adddevice.
        Захотелось собрать LLDP с устройств - идём в другую ручку со списком устройств. А-ля: https://netapi.linkmeup.ru/api/lldp.

        Исключительно как вариант: это может быть приложение на Django, FastAPI, Flask, запущенное как systemd-сервис.
    </li>
    <li>
        <b><u>Набор приложений</u></b>, которые реализуют функционал ручек NetAPI. Например, клиент хочет получить список MAC-адресов со свитча - он идёт в ручку, а ручка дёргает модуль сбора MAC'ов, модуль идёт на свитч по SSH и собирает необходимую информацию (через CLI или NETCONF).
        Это может быть как интегральная часть NetAPI, так и отдельные сервисы, с которыми NetAPI взаимодействует по ещё одному API (REST, GRPC).
    </li>
    <li>
        <b><u>Сервис NetGet</u></b>, выполняющий регулярные и разовые задачи на сбор данных с сетевых устройств, таких как бэкапы, коммиты, версии ПО итд.
        Это может быть systemd-сервис или просто набор скриптов, запускающихся по cron'у или триггеру.
    </li>
    <li>
        <b><u>ConfMan - Configuration Manager</u></b> - это набор сервис и компонентов, выполняющий всю работу по управлению конфигурацией. 
        Его составными частями являются:
        <ul>
            <li>
                HLD - формализованный дизайн сети (High Level Design). Это могут быть объекты того языка программирования, на котором написана система автоматизации, может быть набор YAML-файлов или что-то своё собственное.
            </li>
            <li>
                Хранилище переменных, необходимых для конфигурации, которые по тем или иным причинам не получается хранить в IPAM/DCIM (например, префикс-листы или syslog-сервера).
            </li>
            <li>
                Специфические компоненты, такие как система управления доступами - для ACL. Или система планирования нагрузки для генерирования конфигов QoS-очередей. Возможно, оркестраторы/контроллеры для инжиниринга трафика, тоже стоит рассматривать как часть ConfMan.
            </li>
            <li>
                Набор генераторов конфигурации - то самое, что возьмёт HLD, обогатит его данными из IPAM/DCIM, хранилища, других систем и сформирует конечный вид конфигурации устройства в вендор-специфичном виде.
            </li>
            <li>
                Возможно, часть, которая вычисляет фактическую дельту конфига и формирует патч, то еcть список команд для достижения целевого состояния. <i>Возможно</i> - потому что вместо применения только изменений, можно целиком конфигурацию заменять. 
            </li>
            <li>
                Модуль, отвечающий за сличение целевого и реального конфига.
            </li>
        </ul>
        Отдельные компоненты ConfMan взаимодействуют друг с другом через тот или иной API.
    </li>
    <li>
        <b><u>Carrier</u></b> - доставщик изменений на сеть. Например, ConfMan сгенерировал пачку конфигов и передал Carrier'у на применение.
        В зависимости от используемого интерфейса взаимодействия с сетевым устройством он выполняет разные функции.
        Так, для CLI он знает специфику взаимодействия с консолью конкретного вендора - интерактивные ответы, ошибки, информационные сообщения.
        Для NETCONF'а он умеет определять успешность или неуспешность применения конфигурации.
        <blockquote>
            Можно было бы назвать его worker'ом, но Carrier - это функциональный компонент, тогда как Worker - это его экземпляр. То есть может быть несколько worker'ов, выполняющих задачу Carrier, настраивая одновременно две разные железки.
        </blockquote>
    </li>
    <li>
        Над всем этим царит <b><u>Dispatcher</u></b> - этакий диспетчер задач, бригадир, который распределяет работу. 
        Он ведёт учёт всех поступивших задач, отслеживает их статусы, составляет расписание на исполнение. 
        Например, если стоит задача обновить 300 свитчей, то он знает, что нельзя это делать одновременно, поэтому он составит расписание. Так же он не выведет из эксплуатации больше двух спайнов одновременно, и не проведёт работы на двух бордерах.
        Если на конкретную железку уже есть задача или на ней CPU под сотку, это значит, что применение изменений нужно отложить.
        В общем вот таким составлением расписания и занимается Dispatcher.
    
    
        Все задачи связанные с доступом на сетевое устройство, проходят через него. 
    </li>
</ol>

Вот такая получается система. Не очень простая, но не очень и сложная.

Давайте сразу отметим несколько важных характеристик этой системы.

<a name="PRINICIPLES"></a>
<h2>Характеристики системы</h3>

<a name="API"></a>
<h3>Единый интерфейс</h3>
<b><u>Во-первых</u></b>, отметим здесь центральную роль <b>NetAPI</b>. Он является точкой входа для большинства задач: ввести новое железо, переконфигурить старое, обновить свитч. Внутри задачи могут быть подзадачи, требующие обращение к NetAPI, например, обновление ПО своей подзадачей имеет снятие нагрузки, которое тоже может являться ручкой NetAPI, а снятие нагрузки в свою очередь требует проверки наличия трафика на портах, что тоже подразумевает поход в NetAPI. И так далее.

<a name="ASYNC"></a>
<h3>Асинхронность</h3>
<b><u>Во-вторых</u></b>, нам необходим асинхронный режим работы API. Некоторые из запросов (тот же ввод нового оборудования в работу) может длиться продолжительное время, то есть ответ клиенту не вернётся в обозримое время. Поэтому нужна возможность создать заявку, получить её ID и вернуться позже за уточнением её статуса.
Для этого каждому запросу в API выделяется ID, данные о нём вносятся в базу данных, статус обновляется по мере поступления новых данных.

Соответственно должна существовать отдельная ручка (-и), в которую (-ые) можно прийти и узнать статус запроса по ID.

<a name="ACID"></a>
<h3>ACID</h3>
<b><u>В-третьих</u></b>, применение конфигурации на сеть должно соответствовать принципам ACID.
Давайте рассматривать выкатку новой конфигурации на сеть как транзакцию.
<ul>
    <li>
        <b>A - Atomicity</b>. Никакая конфигурация не должна примениться частично. Как в пределах устройства, так и в периметре сервиса - на наборе устройств. Применяется либо вся конфигурация, либо никакая. Соответственно, если на ряде устройств конфигурация применилась, она должна быть откачена. Либо средствами встроенного rollback-механизма, либо набором отменяющих изменения команд. 
    </li>
    <li>
        <b>C - Consistency</b>. Именно в том виде, как понятие консистентность применяется к БД, к сети, пожалуй, не применима, но мы будем иметь в виду, что все сетевые сервисы после применения новой конфигурации остаются работоспособными.
        Факт консистенстности проверяется набором тестов, запускающимся после выкатки конфигурации. В зависимости от типа изменений могут быть разные наборы тестов. Иногда достаточно проверить CPU на паре коробок, в другой раз запустить пинги и проверить статусы BGP-сессий, а в третьем - всесторонние тесты всего, что настроено на сети.
    </li>
    <li>
        <b>I - Isolation</b>. Вполне понятный принцип применительно к сети - с того момента, как мы запланировали выкатку новой версии и до её применения, статус сети должен быть зафиксирован - никто не должен её менять. И уж тем более никто не должен настраивать что-то одновременно с запланированной выкаткой.
        Но это качество проще обозначить, чем обеспечить. Допустим, все таски внутри системы управляются Диспетчером, и он выстроит все задачи в правильном порядке. Однако как быть с тем, что кто-то может руками наадхочить на железке? Есть только один способ с этим справиться - <b>люди не ходят на оборудование напрямую</b> - Zero Touch Prod, помним. То есть на железе остаётся служебная учётка нашей системы автоматизации и аварийная для инженеров, которую используют только в ситуациях, когда система сложилась и надо срочно попасть на железо.
        Увы, это не отвечает на два вопроса: "А для тшута мы что делаем?" и "Что мешает инженеру пользоваться аварийной учёткой?". Вообще-то и на тот и на другой вопрос можно подобрать ответы, но не будем тут зацикливаться.
    </li>
    <li>
        <b>D - Durability</b>. Ну тут всё просто - что бы ни случилось на сети, после восстановления конфигурация должна быть прежней. Решается это сохранением конфигурации при каждом коммите (или изменении конфиги, если коммита нет). Но есть нюанс - идентичная конфигурация не говорит об идентичном поведении - дело может быть в консистентности FIB. Но это тоже уже за рамками данной статьи.
    </li>
</ul>

<a name="MQ"></a>
<h3>Взаимодействие компонент через API</h3>
<b><u>В-четвёртых</u></b>, взаимодействие между элементами системы. Очевидно на схеме выше лишь упрощённая схема. Фактически она будет значительно больше, а количество связей и сообщений между элементами превысит все мыслимые и немыслимые значения<s>, а Васюки станут центром десяти губерний!</s>.
К чему это я? Взаимодействие между частями системы должно быть реализовано через API, каким бы он ни был - gRPC, HTTP REST, да хоть SOAP (нет, не хоть).
А кроме того, в какой-то момент нам может понадобиться <b>очередь сообщений</b> (<b>Message Queue</b>). Мы всё это ещё потом в контейнеры сложим. И наступит полный микросервис.


Однако, сосредоточимся на важнейшей задаче - снизить нагрузку на инженера, а для этого надо исключить хождение инженеров на железо напрямую - нужно теперь разобрать сценарии, когда это требуется в обычной жизни.
<hr>

<a name="SCENARIOS"></a>
<h2>Сценарии</h2>
    
В реальной жизни их, конечно, будет много. Я же опишу самые необходимые:
<ul>
    <li><a href="#CHECKS">0. Проверка сети</a></li>
    <li><a href="#NEW">1. Ввод нового оборудования</a></li>
    <li><a href="#SOT_TRIGGERED">2. Переконфигурация из-за изменений переменных в инвентарной системе</a></li>
    <li><a href="#DESIGN_TRIGGERED">3. Переконфигурация из-за изменения дизайна</a></li>
    <li><a href="#DATA_COLLECTING">4. Сбор информации с устройств</a></li>
    <li><a href="#DRAIN">5. Снятие и возврат нагрузки</a></li>
    <li><a href="#UPGRADE">6. Обновление ПО</a></li>
    <li><a href="#DELETE">7. Удаление устройства</a></li>
    <li><a href="#REPLACE">8. Замена устройства</a></li>
</ul>


Распишем каждый из них детально.

<a name="CHECKS"></a>
<h3>0. Проверка сети</h3>
Наверно, совершенно оправданным будет нулевой приоритет отдать именно блоку ручек для проверки сети, поскольку к ним нужно будет обращаться почти в каждом последующем сценарии.
Часть из них будут реализовывать blackbox-проверки. Например, наличие e2e-связности, потерь, RTT.
Другая whitebox - существование тех или иных маршрутов в RIB, состояние FIB итд.

<ol>
    <li>
        Человек или сервис приходит в ручку NetAPI с запросом, в теле которого указаны параметры теста. Например, ICMP, устройство-источник, адрес назначения, VRF, число проб, размер пакета.
    </li>
    <li>
        NetAPI формирует запрос в NetGet, чтобы тот собрал данные с сети/устройства. И тот собирает.
    </li>
    <li>
        Результаты теста возвращаются клиенту.
    </li>
</ol>


<a name="NEW"></a>
<h3>1. Ввод нового оборудования</h3>
В зависимости от скорости роста, возможно, следующий самый важный сценарий - быстро запускать новые узлы (стойки, филиалы, офисы), поскольку обычно занимает больше всего времени.
<ol>
    <li>
        Человек или часть системы, реализующей нечто а-ля ZTP, приходит в NetAPI для инициализации устройства.
        Устройство идентифицируется по своему серийнику или инвентарному номеру, и ему должна быть задана роль, чтобы было понятно, с какой конфигурацией его наливать.
        <img src="https://fs.linkmeup.ru/images/adsm/4/step1.svg" width="600">
        <ol>
            <li>
                Ручка дёргает конкретное приложение, отвечающее за этот шаг
            </li>
            <li>
                Приложение создаёт устройство в NetBox и прописывает его
                <ul>
                    <li>Имя</li>
                    <li>Серийник</li>
                    <li>Локацию</li>
                    <li>Вендор/модель</li>
                    <li>Роль в сети</li>
                    <li>Присущие ему свойства: список интерфейсов, консольных портов, комментарии.</li>
                </ul>
            </li>
            <li>
                Приложение определяет и при необходимости создаёт MGMT-интерфейс
            </li>
            <li>
                Приложение выделяет MGMT IP.
            </li>
        </ol>
    
        На данном шаге устройство в минимальном виде заведено в инвентарной системе, и заполнены необходимые для первичной настройки параметры.
    </li>
    <li>
        Далее другая часть процесса, а-ля ZTP, приходит в ручку NetAPI в поисках первоначального конфига
        <img src="https://fs.linkmeup.ru/images/adsm/4/step2.svg" width="700">
        <ol>
            <li>Ручка дёргает конкретное приложение</li>
            <li>Приложение собирает данные из NetBox и, возможно, внешних систем</li>
            <li>Приложение рендерит конфиг, возвращает его клиенту и заодно складывает его в git-репозиторий.</li>
            <li>Клиент каким-то образом доставляет конфигурацию до устройства - это может быть ZTP или пропихивание конфига через консольный порт. Идентификатором устройства тут выступает серийник.</li>
        </ol>
    
        После этого шага появляется удалённый SSH-доступ на устройство.
    
        Теперь по какому-то триггеру запускается конвейер ввода устройства в эксплуатацию.
        Триггером может быть:
        <ul>
            <li>Чьё-то ручное действие - например, нажатие кнопки в интерфейсе - и сигнал в NetAPI.</li>
            <li>Обращение к ручке ввода в NetAPI от системы ZTP после завершения.</li>
            <li>Факт появления доступа по SSH на устройство - например, кроняка пытается доступиться до железки, которая помечена как "для ввода".</li>
        </ul>
    </li>
    <li>
        Заполняются данные в NetBox, которые в дальнейшем будут служить переменными для генерации конфигурации.
        <img src="https://fs.linkmeup.ru/images/adsm/4/step3.svg" width="800">
        <ol>
            <li>
                Система посылает в NetGet запрос на сбор данных о LLDP с данного свитча.
                <ol>
                    <li>Информация о соседях вносится в NetBox, порты связываются друг с другом.</li>
                    <li>При необходимости создаются сабинтерфейсы или интерфейсы добавляются в LAG.</li>
                    <li>Вычисляются (или выделяются) P2P IP-адреса. </li>
                </ol>

                Необходимые изменения выполняются и на соседнем устройстве.

                Этот шаг позволяет, во-первых, подготовить данные для настройки IP-адресов, во-вторых, визуализировать топологию при необходимости, в-третьих, собрать в будущем информацию о BGP-соседях, если на узле используется BGP.
            </li>
            <li>
                Система создаёт набор виртуальных интерфейсов и выделяет IP-адреса.
                Например, loopback'и и VLAN-интерфейсы.
            </li>
            <li>
                Заполняет другие необходимые данные. Например, ASN, IS-IS Network Entity, настройки l2-интерфейсов.
            </li>
        </ol>
    </li>   
    <li>
        Обновление данных в NetBox инициирует запрос в NetAPI на запуск конвейера для вычисления и деплоя новой конфигурации. Это может быть, например, Web-hook, отправленный самим Netbox'ом.
        Речь здесь идёт обо всех устройствах, конфигурация которых меняется в результате ввода новых устройств. Добавляется новый Leaf - поменяется конфигурация Spine.

        <img src="https://fs.linkmeup.ru/images/adsm/4/step4-7.svg" width="700">
    </li>
    <li>
        NetAPI через Диспетчера адресует задачу на ConfMan, который вычисляет вендор-агностик конфигурацию.
        Для этого система берёт формализованную модель конфигурации данных (питоновские объекты, yaml итд) и подставляет в неё данные из NetBox. 
        Результатом может быть словарь, тот же yaml или питоновский объект.
    </li>
    <li>
        Система генерит конфиг для списка устройств. Результатом может быть текст, содержащий последовательность CLI-команд, NETCONF XML, набор объектов для YANG, Protobuf для gNMI.
    </li>
    <li>
        Выполняются лабораторные тесты CI/CD. Они могут быть в симуляторе, вроде Batfish, виртуальном стенде или всамделишной небольшой железной лабе, мимикрирующей под настоящую сеть.

        Даже для типовой операции, вроде описываемого ввода новых, серверов разумно их делать, ведь данные в SoT изменились - и выкатка может разломать сеть.

        Проходят ручные проверки и подтверждения. 
        <blockquote>
            Это немного сколькзий момент. С одной стороны я всё же не верю, что в обозримом будущем на сеть новый конфиг можно катить без человеческого подтверждения, как это давно происходит в мире WEB-приложений.
            С другой - когда изменения катятся на тысячу устройств, пойди глазами всё просмотри. Поэтому всё же CI/CD и канареечные деплои - это то, к чему мы будем стремиться.
        </blockquote>

        Опционально этот шаг может выполняться в git-репозитории. Хотя заставлять человека переходить во внешний относительно основной системы автоматизации сервис - негуманно. Впрочем как первые шаги разработки такой системы - вполне нормально.
    </li>
    <li>
        По факту сгенерированного конфига или полученных апрувов формируется задача в Dispatcher для Carrier'а на доставку и применение конфигурации на сеть.

        <img src="https://fs.linkmeup.ru/images/adsm/4/step8-9.svg" width="700">
    </li>
    <li>
        Диспетчер диспетчеризирует и следит за выполнением каждой конкретной задачи и всей транзакции целиком.
        Он несёт полную ответственность за то, когда выполняется задача и с каким статусом она завершается. 
    </li>
    <li>
        В случае успешной транзакции Диспетчер обращается в ручки NetAPI, чтобы провести ряд тестов, проверяющих две вещи: 
        <ul>
            <li>Новое устройство готово к обслуживанию трафика,</li>
            <li>Сеть при этом не сломалась.</li>
        </ul>

        <img src="https://fs.linkmeup.ru/images/adsm/4/step10-11.svg" width="800">

        Запускаются какие-то пинги. Проверяется маршрутная информация на сети - сравнивается с бейзлайном (например, состояние, как было до деплоя). Последнее предполагает, что мы либо собрали состояние перед обновлением, либо есть некая база данных с временными рядами (TSDB - Time Series Data Base), содержащая срезы исторических данных.
    
        Есть тесты, падение которых вызовет аварию, но операция будет считаться завершённой. А есть те, после которых произойдёт автоматический откат всей транзакции. Лучше не сделать ничего, чем сделать хорошо, но наполовину.
    </li>
    <li>
        В случае успешных тестов в NetBox и/или иных системах проставляются индикаторы успешного ввода, новое устройство заводится в мониторинги и другие системы.
    </li>
    <li>
        С результатами Диспетчер идёт в ручку NetAPI и сообщает, что ввод завершён успешно, либо нет.

        <img src="https://fs.linkmeup.ru/images/adsm/4/step12.svg" width="600">
    </li>
</ol>

Конвейер завершён.

Это весьма упрощённый конвейер, конечно. Здесь опущены шаги, которые могут быть фактически необходимы в реальной жизни: подавления аварийных сообщений, отписывание комментариев в тикеты, возможные проверки и подтверждения целевой конфигурации живыми людьми, всевозможные валидации на каждом шаге. 

<a name="SOT_TRIGGERED"></a>
<h3>2. Переконфигурация из-за изменений переменных в инвентарной системе</h3>
Допустим по какой-то причине данные в нашем SoT поменялись - человек руками дескрипшон на порту изменил или автоматика пересчитала LLDP-соседства или ещё что-то.
Это изменение, которое должно привести к запуску конвейера по вычислению и выкатке новой конфигурации, описанного выше.

Триггером может быть Web-hook от SoT или опять же кроняка, которая следит за изменениями в этом SoT.

NetAPI получает запрос на запуск конвейера для вычисления и деплоя новой конфигурации, как это уже было в предыдущем сценарии. 
Далее повторяются все те же действия, за исключением специфики, присущей вводу новых стоек. Все те же тесты.

Не забываем про версионирование - изменения переменных в SoT фактически ведёт к изменению версии конфигурации сети. Мажорное, минорное или патч - это предмет жарких дискуссий, судьёй которому будет <a href="https://semver.org/">semver</a>.

<a name="DESIGN_TRIGGERED"></a>
<h3>3. Переконфигурация из-за изменения дизайна</h3>
Это может быть как небольшое изменение политики маршрутизации или ACL, так и сравнительно масштабная вещь, такая как добавление нового типа сервиса на всю сеть.
В целом, что относить к дизайну, а что к переменным - вопрос не просто дискуссионный, думаю, он на данный момент не имеет точного ответа. 

Так же вопрос без ответа, в каком виде дизайн должен храниться - питоновские объекты, словарь, yaml, json? Хотел бы знать.
Но допустим, что независимо от формы он хранится в гите. И тогда его изменение легко можно использовать как триггер для запуска конвейера для вычисления и деплоя новой конфигурации, который мы дважды уже тронули выше.

С точки зрения версионирования - инженер, меняющий дизайн и коммитящий изменения в гит, сам определяет насколько это важное обновление. 

<a name="DATA_COLLECTING"></a>
<h3>4. Сбор информации с устройств</h3>
В целом сбором информации занимается NetGet. Как периодическим, так и разовым по запросу.
Поэтому, когда нужно собрать, например, MAC'и с конкретного устройства, клиент идёт в ручку NetAPI, а тот в свою очередь дёргает NetGet.

NetGet формирует задачу для Диспетчера, чтобы Carrier сходил на устройство и собрал необходимую информацию.

Учитывая, что для таких запросов клиент ожидает синхронный режим, Диспетчер должен по возможности прогнать его с высоким приоритетом и быстро вернуть ответ NetGet'у.

Из любопытных идей для оптимизации: NetGet видится очень активноиспользуемым компонентом - вплоть до того, что мониторинг будет ходить в него, чтобы собрать счётчики и состояние сети - и, возможно, ему стоило бы держать открытыми и прогретыми сессии со всем флотом сетевых устройств. С использованием asyncio данные будут собираться просто в мгновение ока. А шардирование сетевых элементов по разным worker'ам позволит не упираться в лимиты.

<a name="DRAIN"></a>
<h3>5. Снятие и возврат нагрузки</h3>
Этот сценарий не является самостоятельным, если мы говорим про окончательное решение вопроса автоматизации - это, скорее, ручка, к которой мы будем обращаться из других сценариев.

С одной стороны это задача, требующая ультра-много операций, занимающая много времени и склонная к человеческим ошибкам. Допустим какой-нибудь бордер вывести из эксплуатации, для замены контрол-бордов. Явно нужно автоматически это делать.
С другой - зачастую это работа, требующая весьма интеллектуальной деятельности - поди разбери в нужном порядке разные сервисы, линки, клиентов. 

Но для сравнительно простых устройств, каковыми являются торы, спайны и суперспайны или один из маршрутизаторов в ISP на резервированном канале, сделать это выглядит несложным.

Это может быть реализовано как две ручки: для снятия нагрузки и для возврата - так и как одна: выполняющая полный цикл.
<ol>
    <li>
        Клиент приходит в ручку NetAPI. А тот запускает конвейер увода нагрузки
    </li>
    <li>
        Приложение определяет список сервисов, которые нужно погасить (L2/L3VPN, базовая маршрутизация, MPLS итд)
    </li>
    <li>
        Приложение формирует список действий, которые нужно совершить.
        Например:
        <ol>
            <li>Плавно увести трафик с помощью BGP gshut community или ISIS overload bit (или ещё чего-то)</li>
            <li>Убедиться в отсутствии трафика на интерфейсах</li>
            <li>Выключить BGP-сессии в нужном порядке (сначала сервисные, потом транспортные)</li>
            <li>Выключить интерфейсы</li>
            <li>Убедиться в отсутствии активных аварий по сервисам.</li>
        </ol>
    </li>
    <li>
        Зафиксировать статус задачи. 
    </li>
</ol>

Клиент может начинать выполнять запланированные работы. Клиентом может быть другой конвейер.

По завершении клиент дёргает ту же ручку для возврата нагрузки - и тогда в обратном порядке выполняются предыдущие действия.
Либо же это отдельная ручка, которая независимо описывает, каким образом для данного типа узлов происходит возврат нагрузки. 
    
<a name="UPGRADE"></a>
<h3>6. Обновление ПО</h3>
Обновление может быть двух видов - требующее прерывания сервисов, и нет.

Соответственно конвейеры для них будут разные. 
Рассмотрим для сложного случая
<ol>
    <li>Клиент приходит в ручку NetAPI.</li>
    <li>Запускается конвейер снятия нагрузки</li>
    <li>
        Запускается конвейер обновления ПО:
        <ol>
            <li>Залить файлы ПО</li>
            <li>Проверить контрольную сумму</li>
            <li>Обновить прошивку, указать загрузочные файлы, перезагрузить устройство и провести иные мероприятия</li>
            <li>После обновления проверить версию ПО</li>
        </ol>
    </li>
    <li>Запустить конвейер возврата нагрузки.</li>
</ol>


<a name="DELETE"></a>
<h3>7. Удаление устройства</h3>
Это весьма частый сценарий. Особенно если рассматривать переезд старого устройства в новую роль или локацию, как удаление и создание нового.
<ol>
    <li>Клиент приходит в NetAPI. Тот дёргает приложение, отвечающее за удаление устройства.</li>
    <li>Приложение проверяет, что нагрузка на устройстве ниже определённого порога.</li>
    <li>Приложение обращается в NetAPI в ручку снятия нагрузки. </li>
    <li>Приложение ищет все зависящие от этого устройства объекты в SoT. Как пример:
        <ul>
            <li>Интерфейсы</li>
            <li>IP-адреса</li>
            <li>Подсети</li>
            <li>Интерфейсы соседних устройств</li>
            <li>P2P-адреса соседних устройств</li>
            <li>Итд.</li>
        </ul>
    </li>
    <li>Приложение удаляет их все.</li>
    <li>
        Изменения в SoT триггерят запуск уже известного нам конвейера. Как вы видите он весьма и весьма универсален.
        Как результат - настройки соседних устройств, относящиеся к удаляемому, так же удаляются в процессе деплоя новой конфигурации.
    
        Само же устройство затирается к заводским настройкам. Кроме того оно удаляется из всех мониторингов и других систем.
    </li>
    <li>
        Устройство удаляется из БД или помечается каким-то образом, если нужно сохранить о нём информацию.
    </li>
</ol>

<a name="REPLACE"></a>
<h3>8. Замена устройства</h3>
Случается, что свитч ломается. Или нужно железку проапгрейдить на новую модель. В общем надо её снять, а новую поставить.
Теоретически это выглядит как два шага: 
<ul>
    <li>Удаление текущего устройства</li>
    <li>Добавление нового</li>
</ul>

Но нам важны несколько вещей:
<ul>
    <li>Имя нового устройства должно быть таким же, как и у прежнего</li>
    <li>Сохранить MGMT IP</li>
    <li>Сохранить и другие атрибуты: лупбэки, вланы, ASN, итд</li>
    <li>Скорее всего, и конфигурацию</li>
</ul>
Не факт, что это всё необходимо, но, скорее всего, так.

Самым простым выглядит в существующей записи поменять минимум вещей - инвентарник, серийник, модель. Но это лишает гибкости и добавляет несколько щекотливых моментов при выводе старой железки.
Кроме того, мне импонирует мысль, что девайс в БД собой олицетворяет не место и роль, а вполне конкретное устройство. И при добавлении в сеть нового свитча или роутера, в DCIM появляется новая запись.

Поэтому я бы всё же рассматривал замену устройства на сети как
<ul>
    <li>Удаление старого устройства</li>
    <li>Добавление нового с определённым набором атрибутов, значение которых хотим зафиксировать, и которые в противном случае определялись/выделялись бы автоматически.</li>
</ul>

При этом процедура удаления, определённая шагом выше, берётся как есть: с удалением артефактов на других устройствах (пересоздадим на втором шаге) и вычисткой конфига с устройства (чтобы, например, оно случайно на новом месте неожиданно не запустилось со старыми адресами и не начало всасывать и блэкхолить трафик).


<hr>
Естественно, сценарии этим не ограничиваются. Их количество, степень автоматизированности и результаты диктуются бизнес-логикой и рациональностью.

Опять же мы тут опускаем вопросы подавления аварийных сообщений, коммита изменений в репы и подобные. 

Но благодаря такому рассуждению мы приходим к пониманию, что здесь важно заложить наиболее общие и переиспользуемые конвейеры, которые станут впоследствии кирпичиками более сложных задач.
Сами конвейеры при этом декомпозируются на ещё более простые и универсальные атомы. 

<hr>

<a name="LINKS"></a>
<h1>Полезные ссылки</h1>
<ul>
    <li><a href="https://github.com/networktocode/awesome-network-automation">Awesome Network Automation</a></li>
    <li><a href="https://dteslya.engineer/network_automaiton_101/">Статья Network Automation 101 от Дмитрия Тесля</a></li>
    <li><a href="https://github.com/juliogomez/netdevops">Hands-on with NetDevOps</a></li>
    <li><a href="https://www.youtube.com/watch?v=ErmhE_wmNo0">Scaling the Facebook backbone through Zero Touch Provisioning (ZTP)</a></li>
</ul>

<hr>

<h1>Заключение</h1>
Итак, скромной задачей этой статьи было спуститься на один уровень абстракции ниже по сравнению с <a href="https://linkmeup.ru/blog/424.html">нулевой публикацией</a> и попытаться декомпозировать жизненный цикл оборудования на понятные блоки.

<img src="https://fs.linkmeup.ru/images/adsm/4/dasha.png" width="600">

Повторюсь в очередной раз, что это лишь один из возможных подходов, который просто пришёл в голову мне. Он не только не претендует на оптимальность и проработанность, но и, хуже того, скорее всего, даже на таком описательном уровне уже местами переусложнён. 

И ещё хуже то, что в рамках этой серии крайне маловероятно, что я сделаю что-то практическое, хотя бы отдалённое напоминающее описанную схему.
Хотя зарекаться не буду.

В следующих статьях мы поразбираемся с ZTP и какими-то базовыми скриптами автоматизации. А ещё рано или поздно углубимся в интерфейсы взаимодействия с сетевыми коробками: NETCONF/YANG, gNMI, GRPC. 

<hr>

<h1>Спасибы</h1>
<ul>
    <li><a href="https://www.linkedin.com/in/roman-gorge-2b15896b/?originalSubdomain=se">Роману Горге</a> - бывшему ведущему подкаста linkmeup, а ныне эксперту в области облачных платформ - за комментарии про подход IaC и концепцию ACID применительно к сети.</li>
    <li><a href="https://github.com/arefiev">Михаилу Арефьеву</a> - руководителю проектов по сетевой автоматизации в Яндексе - за анализ и критику архитектуры решения и сценариев.</li>
    <li><a href="https://twitter.com/dmfigol">Дмитрию Фиголю</a> - Network Automation Architect at Cisco Global Demo Engineering - за острые замечания и дискуссию.</li>
    <li><a href="https://www.linkedin.com/in/nikita-astashenko-25a16683/">Никите Асташенко</a> - моему другу и классному разработчику - за поездку на Алтай и долгие разговоры у костра, без которых эта идея не вызрела бы.</li>

</ul>

<blockquote>
    Особо благодарных просим задержаться и пройти на <a href="https://www.patreon.com/linkmeup?ty=h" target="_blank">Патреон</a>.
    <a href="https://www.patreon.com/linkmeup?ty=h" target="_blank"><img src="https://fs.linkmeup.ru/images/patreon.jpg" width="400"></a>
</blockquote>