Итак, сеть спроектирована, IPAM запущен. И вот-вот начнут съезжаться миллионы наших стоек. Будем готовиться к этому.
Мы всё дальше от фантазий и абстрактных разговоров и ближе к практике.
И всё же снова сделаем отступление. Большое дело начинается с большого перекура.

Сеть полезно представлять, как некое единое целое, которое мы переводим из одного состояния в другое. Сервис мы внедряем на всей сети. Не может быть такого, что он работает только на 3 устройствах из 4 необходимых. Вся сеть должна обеспечивать отказоустойчивость и достаточную полосу.
Однако рано или поздно всё равно любая задача декомпозируется до уровня отдельных сетевых коробок.
И если про сеть как единый организм мы уже поговорили  в 0-й статье!!!, то пришло время разобраться отдельными органами.

В этой статье разберём жизненный цикл сетевого устройства и некоторые сценарии того, какие манипуляции с ним приходится порой делать.
Естественно, всё это интересует нас с точки зрения автоматизируемости. 

<cut>


==Общий взгляд на жизненный цикл оборудования


Проследим её жизнь с первого и до последнего дня.
    1. **Day 0** - железка только появилась в наших руках. Сейчас самое важное - базовая настройка:
        a. Добавить IP-адрес управления и маршрут
        b. Включить SSH
        c. Создать пользователя с правами настройки
    
    Иными словами задача Day0 конфигурации - организовать доступ на устройство.
    
    2. **Day 1** - Железка уже встала на позицию, определена его роль в сети и сервисы, которые она обслуживает. 
    Теперь нужно настроить уже целевую конфигурацию, с которой устройство встанет в сеть под нагрузку.
    
    3. **Day N** - Изменения конфигурации в процессе эксплуатации.
    Бывает мы добавляем новый сервис, пересматриваем дизайн или на худой конец нужно ACL поправить.
    Такие изменения нужно тоже уметь довозить до устройства.
    
    4. **Обслуживание** - Помимо нормальной работы есть периоды, когда устройство нужно аккуратно вывести из под нагрузки, чтобы, например, поменять в нём плату, провести обновление ПО.
    
    5. **Отслеживание изменений** - со всей сети следует собирать информацию о том, где и во сколько применялась новая конфигурация. Это позволит как скоррелировать жалобы клиентов с изменениями, так и знать, когда применялась новая конфигурация в обход процедуры.
    
    6. **Проверка соответствия эталонной конфигурации** - В течение всей жизни устройства нужно проверять, что его конфигурация не разошлась с целевой из-за сбоев в автоматике, обновлений ПО или прямого вмешательства чьих-то рук. 
    
    7. **Бэкапы** - Даже если мы в любой момент можем сгенерировать эталонную конфигурацию, чтобы применить её на устройство, бэкапы необходимы.
    
    8. **The last day** - снятие нагрузки, удаление из всех систем, ритуальное сжигание.
        
Я намеренно обхожу в этой статье вопрос мониторингов операционного состояния и реакции на них, поскольку её лейтмотив - это всё же конфигурация.

<hr>
Далее обсудим каждую из задач более детально.


==Day0
Итак, поставщик привёз на склад новый свитч. Его нужно установить настроить, проверить, запустить трафик, добавить во все системы: инвентарные, мониторинги, бэкапы, скрипты автоматизации.
Но всё же самое первое - это получить на него SSH - всё остальное - наживное.

Day 0 делится на две задачи: 
    - Завести устройство в системах
    - Настроить базовый доступ 
Говорить про них в отрыве друг от друга сложно и делать мы так не будем.

Какие же есть способы?

    1. **Бумер**  - вручную завести устройство в инвентарной системе и выделить свободный IP-адрес. Пусть это будет даже экселька.
    Подключить свитч к компьютеру и через консольный порт настроить IP-адрес, маршрут, включить SSH, создать пользователя.
    Отвезти свитч на позицию.
    + Просто, не требует почти никакой инфраструктуры.
    - Склонно к ошибками, масштабируется человекочасами. 

    2. **Бумер+** - автоматизируем заведение устройства в DCIM/IPAM. Мы только нажимаем кнопочку, а в системе появляется железка на правильной локации со всеми нужными портами, ей выделяется автоматически имя и следующий свободный IP-адрес. В итоге генерируется базовый конфиг в виде текстового файлика.
    Администратор подключает свитч к компьютеру и через консольный порт копипастит содержимое этого файлика в терминал.
    + Ниже вероятность ошибок, значительно меньше ручной работы
    - Требуется уже какая-никакая инфраструктура: IPAM/DCIM с API, скрипт, всё ещё ручная работа, всё ещё настраивать на стенде и потом везти устройство на позицию.

    3. **Миллениал** -  ZTP - Zero Touch Provisioning - подход, которому 100 лет в обед, но он почему-то всё ещё есть не везде. Идея в том, что устройство сразу же ставится на позиции и подключается в сеть управления, после чего по DHCP оно само получает свою конфигурацию. 
    Для этого устройство должно быть уже заведено в IPAM/DCIM и предгенерирована конфигурация, которая и передаётся устройству. 
    + Устройство можно сразу везти на позицию, минимум ручного труда
    - Нужна уже продуманная связная инфраструктура: IPAM/DCIM, DHCP, (T)FTP, автогенерация конфигов. Сложно реализовать для распределённых сетей, вроде ритейла.
    
    4. **Зумеры** - SD-WAN. Кстати, как раз подходит для ритейлов, хотя в свою очередь не очень для датацентров. Подход разделяет идею ZTP - мы устройство включаем, а оно само настраивается.
    + Меньше вероятность ошибок. На первый взгляд меньше работы
    - Однако SD-WAN - это преимущественно проприетарные решения вендоров, требующие мощной инфраструктуры, причём иногда только в облаке вендора. У нас, кстати, был целый подкаст про SD-WAN: (https://linkmeup.ru/blog/588.html - telecom №91. SD-WAN)
    
    5. **Пост-хипстеры** - 
    В Яндексе у нас помимо Out of Band сети управления, есть ещё консольное соединение до абсолютно каждой железки. Для этого есть соответственно сеть консольных серверов внутри датацентров и точек присутствия.
    Каждое новое устройство после установки подключается отдельно в OOB-свитч по Ethernet и в консольный сервер консольным линком.
    Это позволяет реализовать схему, подобную описанной ниже:
    - Устройство добавляется в IPAM/DCIM
    - Устройство устанавливается и подключается по управлению
    - Инженер в ДЦ создаёт задачу на сервер наливки: настроить свитч за консольными сервером №7, порт 3.
    - Сервер наливки подключается на указанный порт, забирает серийный номер, с которым идёт в IPAM, генерирует базовый конфиг и обратно через тот же консольный порт применяет данную конфигурацию.
    + Всегда есть консольный доступ на устройство, что бы ни происходило в сети трафика и управления. Нет проблем с вендорскими особенностями - консольный протокол у всех реализован одинаково (с поправкой на параметры порта)
    - Совсем непросто и в абсолютных цифрах недёшево реализовывать ещё одну сеть управления. Не подходит для географически распределённых сетей. Требуется серьёзная инфраструктура даже в минимальном варианте без использования сервера наливки.


Как видите, любые решения по автоматизации Day 0 требуют чего-то больше, чем просто скриптик на питоне. К этому процессу нужно подходить системно с точки зрения выстраивания инфраструктуры. 

Так или иначе эта часть автоматизирована у многих, потому что подходы понятны, инструменты в ассортименте.
<hr>

==Day1
Тут уже заметно интереснее. Одно дело - сгенерировать простейший конфиг на 20 строчек, одинаковый для всех типов устройств, как было в Day 0, и совсем другое - целевой конфиг на пару тысяч строк, который может радикально отличаться от железки к железке в зависимости от её роли и необходимых сервисов. Например, конфигурации одной и той же модели свитча, установленных в качестве лифа и спайна, будут различаться как минимум настройками даунлинк интерфейсов. 
Основная идея здесь в том, что мы описываем дизайн сети в том или ином формальном виде и отдаём его генераторам. Генераторы берут этот дизайн, роль устройства, локацию, переменные из IPAM/DCIM, всё это перемешивают, а на выходе получается специфический для данной коробки конфиг.
То есть основных компонента здесь два:
    1) Формализованный дизайн
    2) Заполненные данные в IPAM/DCIM

Здесь подробно останавливаться не будем - формализации дизайна я посвящу отдельный (и скорее всего не один) выпуск.

Итак, имеем конфиг Day1. Осталось всего ничего - применить его на железку. 
И тут все средства хороши в разных комбинациях: CLI, SSH, netmiko, NETCONF, GNMI, REST API, консоль, SNMP (я сейчас не шучу - лично видел), FTP, SCP.
В целом на нерабочую пока железку применить конфиг действительно можно разными способами:
    - Ручной копипаст из файлика в терминал
    - Применение команд последовательно через SSH из кода, используя тот же netmiko 
    - Копирование файла на флэшку устройства, установка его в качестве конфигурационного и ребут железки
    - А-ля config replace
    - Пульнуть через NETCONF весь конфиг в XML
    - gNMI

Об этом тоже ещё поговорим. 

С автоматизацией этой задачи большинство тоже справляются - один раз настроить железку без нагрузки - дело нехитрое. 


Если есть процесс и инструменты Configuration Management и версионирования конфигурации, то Day1  - это лишь частный случай DayN.


==Day N

А вот с этим дела обстоят туго чуть менее, чем у всех. Говоря это, я не шучу. Тут всё плохо.
Дело в том, что нагенерить конфигурацию - действительно несложно. Пусть это будет даже циклопический jinja-шаблон.

А вот применить этот конфиг на железку ещё и под продуктивной нагрузкой - цель для инженеров со стальными нервами.

Тут целый ком проблем, как очевидных, так и неявных.

Во-первых, интерфейс: CLI, NETCONF, GNMI, SCP/FTP.
Если CLI - то как быть с особенностями реализации каждого вендора? Режимы контекстов, интерактивные диалоги, порядок выполнения команд.
Если NETCONF или gNMI - то его не все вендоры поддерживают. А те, кто поддерживает, делают это сильно по-разному, и зачастую не в полной мере. А если в полной мере, то, конечно, же в своей схеме, а не в OpenConfig.
А если файлик подложить - то не все на лету умеют заменять, а значит с ребутом - только кому он нужен при добавлении BGP-пира?

Во-вторых, инструмент доставки: netmiko, ncclient, ansible (какой модуль)?

В-третьих, как заливать вслепую? Отправляя полную конфигурацию, мы не знаем, как она изменит состояние устройства. Даже если мы видим диф между файлами или в ветке в гите, это не говорит о том, какие команды фактически применятся на железке.

В-четвёртых, даже если мы видим будущие изменения (кандидат-конфиг на самом устройстве, к примеру), то это не говорит о том, что мы ничего не разломаем по своей неосмотрительности. Тут уже напрашивается сетевой CI/CD.

В-пятых, весь ворох вопросов мультивендорной взрослой сети: разный синтаксис, семантика даже между версиями софта, где-то есть коммиты, где-то нет, где-то можно увидеть кандидат, где-то нет.
<hr>

Это область компромиссов. 

Но давайте будем честны сами с собой: с вероятностью процентов 80 вам не нужен выстроенный процесс версионирования конфигурации, конвейер CI/CD и автоматическая выкатка. 
Скорее всего, вам действительно достаточно залить первичный конфиг, а дальше новые изменения элементарными плейбуками накатывать всю жизнь. И для этого, включая мониторинги и внутренние инструменты, достаточно 2-5 человек, а не целый штат разработчиков.
И большинство компаний именно так и делает. 
Можно добавить git-lab, Teamcity, AWX, программную или аппаратную лабораторию с набором тестов. Это всё мощные улучшайзеры, которые сделают процесс выкатки новой конфигурации значительно безопаснее. Но они не переведут управление конфигурацией на принципиально новый уровень.

А мы ведь всё же хотим
    - Универсальное решение
    - Максимальную автоматизацию рутины
    - Безопасные выкатки конфигурации
    - Формализованный дизайн
    - Версионирование
    - Транзакционность


Поэтому давайте составим схему перспективной системы автоматизации, которая позволит нам решить все задачи. 


==С высоты птичьего полёта
Один из принципов, который нужно заложить в систему - это Zero Touch Prod, то есть свести к минимуму прямое хождение инженера на устройства. Любые изменения конфигурации только через платформу, только через  интерфейс.

Итого, какой список задач решаем?
    1) Нужен интерфейс, через который можно создавать задачи. Он абстрагирует работу с сетевыми устройствами. 
    Графический - для инженеров, API - для внешних сервисов. 
    2) Ввод новых устройств
    3) Актуализация данных в инвентарной системе (LLDP, список интерфейсов, IP-адресов)
    4) Генерация целевых конфигураций
    5) Применение целевых конфигураций и временных патчей (тшут, костыль)
    6) Сличение целевых и реальных конфигов
    7) Снятие и возврат нагрузки
    8) Обновление ПО
    9) Сбор бэкапов, коммитов, 
    10) Диспетчеризация задач, выполняющихся на железе.
    

Соответственно схематично я бы изобразил это так:

Bird_scheme!!!

    1. IPAM/DCIM- система, являющаяся Source of Truth для всей системы автоматизации. В нашем случае - Netbox.
    2. NetAPI - служба одного окна. Что бы ни вздумалось сделать с сетью - идём в него. 
    Например, захотелось добавить новый свитч - идём в ручку NetAPI с нужным набором параметров (например, серийник, имя, локация) и создаём задачу на добавление свитча. А-ля: https://netapi.linkmeup.ru/api/adddevice.
    Захотелось собрать LLDP с устройств - идём в другую ручку со списком устройств. А-ля: https://netapi.linkmeup.ru/api/lldp.

    Исключительно как вариант: это может быть приложение на Django, Fastapi, Flask, запущенное как Systemd-сервис.

    2. Набор приложений, которые реализуют функционал ручек NetAPI. Например, клиент хочет получить список MAC-адресов со свитча - он идёт в ручку, а ручка дёргает модуль сбора MAC'ов, модуль идёт на свитч по SSH и собирает необходимую информацию (через CLI или NETCONF).
    Это может быть как интегральная часть NetAPI, так и отдельные сервисы, с которыми NetAPI взаимодействует по ещё одному API (REST, GRPC).

    3. Сервис NetGet, выполняющий регулярные и разовые задачи на сбор данных с сетевых устройств, таких как бэкапы, коммиты, версии ПО итд.
    Это может быть SystemD-сервис или просто набор скриптов, запускающихся по cron'у или триггеру.
    
    4. ConfMan - Configuration Manager - сервис, выполняющий всю работу по управлению конфигурацией. 
    Его составными частями являются:
        ○ HLD - формализованный дизайн сети. Это могут быть объекты того языка программирования, на котором написана система автоматизации, может быть набор YAML-файлов или что-то своё собственное.
        ○ Хранилище переменных, необходимых для конфигурации, которые по тем или иным причинам не получается хранить в IPAM/DCIM (например, список ACL).
        ○ Набор генераторов конфигурации - то самое, что возьмёт HLD, обогатит его данными из IPAM/DCIM и хранилища и сформирует конечный вид конфигурации устройства.
        ○ Возможно часть, которая вычисляет фактическую дельту конфига и формирует патч, то еть список команд для достижения целевого состояния. Актуально в случае CLI.
        ○ Модуль, отвечающий за сличение целевого и реального конфига
    
    5. Carrier - доставщик изменений на сеть. Например, ConfMan сгенерировал пачку конфигов и передал Carrier'у на применение.
    В зависимости от используемого интерфейса взаимодействия с сетевым устройством он выполняет разные функции.
    Так, для CLI он знает специфику взаимодействия с консолью конкретного вендора - интерактивные ответы, ошибки, информационные сообщения.
    Для NETCONF'а он умеет определять успешность или неуспешность применения конфигурации.
    
    6. Над всем этим парит Dispatcher - этакий диспетчер задач, бригадир, который распределяет работу. 
    Он ведёт учёт всех поступивших задач, отслеживает их статусы, составляет расписание на исполнение. 
    Например, если стоит задача обновить 300 свитчей, то он знает, что нельзя это делать одновременно, поэтому он составит расписание. Так же он не выведет из эксплуатации больше двух спайнов одновременно, и не проведёт работы на двух бордерах.
    Если на конкретную железку уже есть задача или на ней CPU под сотку, это значит, что применение изменений нужно отложить.
    В общем вот таким составлением расписания и занимается Dispatcher.
    
    
    Все задачи связанные с доступом на сетевое устройство, проходят через него. 
    

Вот такая получается система. Не очень простая, но не очень и сложная.

Хочется отметить здесь центральную роль NetAPI. Он является точкой входа для большинства задач.
Он логирует все запросы и выполненные действия, статусы либо же заносит их в какую-то базу данных

Более того, следует здесь иметь в виду, что некоторые из запросов (тот же ввод нового оборудования в работу) может длиться продолжительное время, то есть ответ клиенту не вернётся синхронно. Скорее всего, понадобится возможность создать заявку, получить её ID и вернуться позже за уточнением её статуса.
Поэтому запись в БД всего, что касается каждой задачи, выглядит более чем разумно.

И, вспоминая одну из важнейших задач - исключить хождение инженеров на железо напрямую - нужно теперь разобрать сценарии, когда это требуется в обычной жизни.

==Сценарии
    
В реальной жизни их, конечно, будет много. Я же опишу самые необходимые:
    - Ввод нового оборудования
    - Переконфигурация из-за изменений переменных в инвентарной системе
    - Переконфигурация из-за изменения дизайна
    - Сбор какой-либо информации с устройств
    - Снятие и возврат нагрузки
    - Обновление ПО
    - Удаление устройства
    - !!!
    

Распишем каждый из них детально.

===Ввод нового оборудования
В зависимости от скорости роста, возможно самый важный сценарий - быстро запускать новые узлы (стойки, филиалы, офисы), поскольку обычно занимает больше всего времени.

    1. Человек или часть системы, реализующей нечто а-ля ZTP, приходит в NetAPI для инициализации устройства.
        a. Ручка дёргает конкретное приложение, отвечающее за этот шаг
        b. Приложение создаёт устройство в NetBox и прописывает его
            i. Имя
            ii. Серийник
            iii. Локацию
            iv. Вендор/модель
            v. Роль в сети
            vi. Присущие ему свойства: список интерфейсов, консольных портов, комментарии.
        c. Приложение определяет и при необходимости создаёт MGMT-интерфейс
        d. Приложение выделяет MGMT IP.
    
    На данном шаге устройство в минимальном виде заведено в инвентарной системе, и заполнены необходимые для первичной настройки параметры.
    2. Далее другая часть процесса а-ля ZTP приходит в ручку NetAPI в поисках первоначального конфига
        a. Ручка дёргает конкретное приложение
        b. Приложение собирает данные из NetBox и, возможно, внешних систем
        c. Приложение рендерит конфиг, возвращает его клиенту и заодно складывает его в git-репозиторий.
        d. Клиент каким-то образом доставляет конфигурацию до устройства - это может быть ZTP или пропихивание конфига через консольный порт.
    
    После этого шага появляется удалённый SSH-доступ на устройство.
    
    Теперь по какому-то триггеру запускается конвейер ввода устройства в эксплуатацию.
    Триггером может быть:
        - Чьё-то ручное действие - например, нажатие кнопки в интерфейсе.
        - Обращение к ручке ввода в NetAPI от системы ZTP после завершения.
        - Факт появления доступа по SSH на устройство - например, кроняка пытается доступиться до железки, которая помечена как "для ввода".
        
    3. Заполняются данные в NetBox, которые в дальнейшем будут служить 
        A. Система посылает в NetGet запрос на сбор данных о LLDP с данного свитча.
            i. Информация о соседях вносится в NetBox, порты связываются друг с другом.
            ii. При необходимости создаются сабинтерфейсы или интерфейсы добавляются в LAG.
            iii. Вычисляются (или выделяются) P2P IP-адреса. 

        Необходимые изменения выполняются и на соседнем устройстве.

        Этот шаг позволяет, во-первых, подготовить данные для настройки IP-адресов, во-вторых, визуализировать топологию при необходимости, в-третьих, собрать в будущем информацию о BGP-соседях, если на узле используется BGP.
        B. Система создаёт набор виртуальных интерфейсов и выделяет IP-адреса.
        Например, loopback'и и VLAN-интерфейсы.
        C. Заполняет другие необходимые данные. Например, ASN, IS-IS Network Entity, настройки l2-интерфейсов.
        
    4. Обновление данных в NetBox инициирует запрос в NetAPI на запуск конвейера для вычисления и деплоя новой конфигурации. Это может быть, например, Web-hook, отправленный самим Netbox'ом.
    Речь здесь идёт обо всех устройствах, конфигурация которых меняется в результате ввода новых устройств.
    
    5. NetAPI адресует задачу на ConfMan, который вычисляет вендор-агностик конфигурацию.
    Для этого система берёт некую формализованную модель конфигурации данных (питоновские объекты, yaml итд) и подставляет в неё данные из NetBox. 
    Результатом может быть словарь, тот же yaml или питоновский объект.
    
    6. Система генерит конфиг  для списка устройств. Результатом может быть текст, содержащий последовательность CLI-команд, NETCONF XML, набор объектов для YANG, JSONина для gNMI.
    
    7. Опционально: только что сгенерированные конфиги сохраняются и коммитится в git-репозиторий. Проходят апрувы.
    
    8. По факту сгенерированного конфига или полученных апрувов формируется задача в Dispatcher для Carrier'а на доставку и применение конфигурации на сеть.
    
    9. Диспетчер диспетчеризирует и следит за выполнением каждой конкретной задачи и всей транзакции целиком.
    Он целиком и полностью несёт ответственность за то, когда выполняется задача и с каким статусом она завершается. 
    
    10. С результатами Диспетчер идёт в ручку NetAPI и сообщает, что ввод завершён успешно, либо нет.
    
    11. В случае успеха приложение за NetAPI проводит ряд тестов, проверяющих две вещи: 
        A. Новое устройство готово к обслуживанию трафика
        B. Сеть при этом не сломалась.
    
    Запускаются какие-то пинги, проверяется маршрутная информация - сравнивается с тем, что было до деплоя (или с бейзлайном).
    
    Есть тесты, падение которых вызовет аварию, но операция будет считаться завершённой. А есть те, после которых произойдёт автоматический откат всей транзакции. Лучше не сделать ничего, чем сделать хорошо, но наполовину.
    
    12. В NetBox и/или иных системах проставляются индикаторы успешного ввода, новое устройство заводится в мониторинги и другие системы.
    
    13. Конвейер завершён.

Это весьма упрощённый конвейер, конечно. Здесь опущены шаги, которые могут быть фактически необходимы в реальной жизни: подавления аварийных сообщений, отписывание комментариев в тикеты, возможные проверки и подтверждения целевой конфигурации живыми людьми, всевозможные валижации на каждом шаге. 


==Переконфигурация из-за изменений переменных в инвентарной системе
Допустим по какой-то причине данные в нашем SoT поменялись - человек руками дескрипшн на порту изменил или автоматика пересчитала LLDP-соседства или ещё что-то.
Это изменение, которое должно привести к запуску конвейера по вычислению и выкатке новой конфигурации.

Триггером может быть Web-hook от SoT или опять же кроняка, которая следит за изменениями в этом SoT.

NetAPI получает запрос на запуск конвейера для вычисления и деплоя новой конфигурации, как это уже было в предыдущем сценарии. 
Далее повторяются все те же действия, за исключением специфики, присущей вводу новых стоек. Все те же тесты.

Не забываем про версионирование - изменения переменных в SoT - это минорное изменение версии.

==Переконфигурация из-за изменения дизайна
Это может быть как небольшое изменение политики маршрутизации или ACL, так и сравнительно масштабная вещь, такая как добавление нового типа сервиса на всю сеть.
В целом, что относить к дизайну, а что к переменным - вопрос не просто дискуссионный, думаю, он на данный момент не имеет точного ответа. 

Так же вопрос без ответа, в каком виде дизайн должен храниться - питоновские объекты, словарь, yaml, json? Хотел бы знать.
Но допустим, что независимо от формы он хранится в гите. И тогда его изменение легко можно использовать как триггер для запуска конвейера для вычисления и деплоя новой конфигурации, который мы дважды уже тронули выше.

<blockquote>
Впрочем, тут возможна специфика: изменения дизайна несут риски, поэтому неплохо бы добавить шаг проведения тестов в лабе с помощью CI/CD. 
</blockquote>

С точки зрения версионирования - инженер, меняющий дизайн и коммитящий изменения в гит, сам определяет насколько это важное обновление. 


==Сбор какой-либо информации с устройств
В целом сбором информации занимается NetGet. Как периодическим, так и разовым по запросу.
Поэтому, когда нужно собрать, например, MAC'и с конкретного устройства, клиент идёт в ручку NetAPI, а тот в свою очередь дёргает NetGet.

NetGet формирует задачу для Диспетчера, чтобы Carrier сходил на устройство и собрал необходимую информацию.

Учитывая, что для таких запросов клиент ожидает синхронный режим, Диспетчер должен по возможности прогнать его с высоким приоритетом и быстро вернуть ответ NetGet'у.

==Снятие и возврат нагрузки
С одной стороны это задача, требующая ультра-много операций, занимающая много времени и склонная к человеческим ошибкам. Допустим какой-нибудь бордер вывести из эксплуатации, для замены контрол-бордов. Явно нужно автоматически это делать.
С другой - зачастую это работа, требующая весьма интеллектуальной деятельности - поди разбери в нужном порядке разные сервисы, линки, клиенты. 

Но для сравнительно простых устройств, каковыми являются торы, спайны и суперспайны или один из маршрутизаторов в ISP на резервированном канале, сделать это выглядит несложным.

Это может быть реализовано как две ручки: для снятия нагрузки и для возврата - так и как одна: выполняющая полный цикл.

    1. Клиент приходит в ручку NetAPI. А тот запускает конвейер увода нагрузки
    2. Приложение определяет список сервисов, которые нужно погасить (L2/L3VPN, базовая маршрутизация, MPLS итд)
    3. Приложение формирует список действий, которые нужно совершить.
    Например:
        A. Плавно увести трафик с помощью BGP gshut comminuty или ISIS overload bit (или ещё чего-то)
        B. Убедиться в отсутствии трафика на интерфейсах
        C. Выключить BGP-сессии в нужном порядке (сначала сервисные, потом транспортные)
        D. Выключить интерфейсы
        E. Убедиться в отсутствии активных аварий по сервисам.
    4. Зафиксировать статус задачи. 

Клиент может начинать выполнять запланированные работы. Клиентом может быть другой конвейер.

По завершению клиент дёргает ту же ручку для возврата нагрузки - и тогда в обратном порядке выполняются предыдущие действия.
Либо же это отдельная ручка, которая независимо описывает, каким образом для данного типа узлов происходит возврат нагрузки. 
    

==Обновление ПО
Обновление может быть двух видов - требующее прерывания сервисов, и нет.

Соответственно конвейеры для них будут разные. 
Рассмотрим для сложного случая
    1. Клиент приходит в ручку NetAPI. 
    2. Запускается конвейер снятия нагрузки
    3. Запускается конвейер обновления ПО:
        A. Залить файлы ПО
        B. Проверить контрольную сумму
        C. Обновить прошивку, указать загрузочные файлы, перезагрузить устройство и провести иные мероприятия
        D. После обновления проверить версию ПО
    4. Запустить конвейер возврата нагрузки.


==Удаление устройства
Это весьма частый сценарий. Особенно если рассматривать переезд старого устройства в новую роль или локацию, как удаление и создание нового.

    1. Клиент приходит в NetAPI. Тот дёргает приложение, отвечающее за удаление устройства.
    2. Приложение проверяет, что нагрузка на устройстве ниже определённого порога.
    3. Приложение обращается в NetAPI в ручку снятия нагрузки. 
    4. Приложение ищет все зависящие от этого устройства объекты в SoT. Как пример:
        - Интерфейсы
        - IP-адреса
        - Подсети
        - Интерфейсы соседних устройств
        - P2P-адреса соседних устройств
        - Итд.
    5. Приложение удаляет их все.
    6. Изменения в SoT триггерят запуск уже известного нам конвейра. Как вы видите он весьма и весьма универсален.
    Как результат - настройки соседних устройств, относящиеся к удалённому, удаляется в процессе деплоя новой конрфигурации.
    
    Само же устройство затирается к заводским настройкам. Кроме того оно удаляется из всех мониторингов и других систем.
    
Опять же мы тут опускаем вопросы подавления аварийных сообщений, коммита изменений в репы и подобные. 


---
Естественно, сценарии этим не ограничиваются. Их количество, степень автоматизированности и результаты диктуются бизнес-логикой.

Но благодаря такому рассуждению мы приходим к пониманию, что здесь важно заложить наиболее общие и переиспользуемые конвейеры, которые станут впоследствии кирпичиками более сложных задач.
Сами конвейеры при этом декомпозируются на ещё более простые и универсальные атомы. 



----

==Заключение
Итак, скромной задачей этой статьи было спуститься на один уровень абстракции ниже по сравнению с первой публикацией !!!! и попытаться декомпозировать жизненный цикл оборудования на понятные блоки.
Dasha!!!

Повторюсь в очередной раз, что это лишь один из возможных подходов, который просто пришёл в голову мне. Он не только не претендует на оптимальность и проработанность, но и, хуже того, скорее всего, даже на таком описательном уровне уже местами переусложнён. 

И ещё хуже то, что в рамках этой серии крайне маловероятно, что я сделаю что-то практическое, хотя бы отдалённое напоминающее описанную схему.
Хотя зарекаться не буду.

В следующих статьях мы поразбираемся с ZTP и какими-то базовыми скриптами автоматизации. А ещё рано или поздно углубимся в интерфейсы взаимодействия с сетевыми коробками: NETCONF/YANG, gNMI, GRPC. 







