Предыдущая статья рассматривала архитектуру виртуализированной сети, underlay-overlay, путь пакета между VM и прочее. В данной статье мы затронем (или попытаемся затронуть) вопросы а как собственно происходит виртаулизация сетевых функций, как реализован backend основных продуктов обеспечивающих запуск и управление VM, а также как работает виртуальный свитчинг (OVS и Linux bridge).

Тема виртуализации широка и глубока, объяснить все детали работы гипервизора невозможно (да и не нужно). Мы ограничимся минимальным набором знаний необходимым для понимания работы любого виртуализированного решения, не обязательно Telco.

<h1>Содержание</h1>
<ul>
    <li><b><a href="#INTRODUCTION">Введение и история краткая виртуализации</a></b></li>
    <li><b><a href="#RESOURCES">Типы виртуальных ресурсов - compute, network, storage</a></b></li>
    <li><b><a href="#SWITCHING">Виртуальная коммутация</a></b></li>
    <li><b><a href="#INSTRUMENTS">Инструменты и технология виртуализации - libvirt, qemu-kvm и прочее</a></b>
    <li><b><a href="#PACKETPATH">Путь пакета в виртуальной сети</a></b>
    <li><b><a href="#SDN">Использование виртуальной коммутации в Cloud платформах</a></b>
    <li><b><a href="#PROBLEMS">Известные проблемы и сложности виртуализации </a></b>
</ul>
<hr>

<li><a name="INTRODUCTION"></a>
<h1>Введение и краткая история виртуализации</h1>

История современных технологий виртуализации берет свое начало в 1999 году, когда молодая компания VMWare выпустила продукт под названием VMWare Workstation. Это был продукт обеспечивающий виртуализацию desktop/client приложений. Виртуализация серверной части пришла несколько позднее в виде продукта ESX Server который в дальнейшем эволюционировал в ESXi (i означает integrated) - это тот самый продукт, который используется повсеместно как в IT так и в Telco как гипервизор серверных приложений.

На стороне Opensource два основных проекта принесли виртуализацию в Linux:
<ul>
    <li>KVM (Kernel-based Virtual Machnine) - модуль ядра Linux который позволяет kernel работать как гипервизор (создает необходимую инфраструктуру для запуска и управления VM). Был добавлен в версии ядра 2.6.20 в 2007 году</li>
    <li>QEMU (Quick Emulator) - непосредственно эмулирует железо для виртуальной машины (CPU, Disk, RAM, что угодно включая USB порт) и используется совместно с KVM для достижения почти "native" производительности</li>
</ul>

<blockquote>
На самом деле на сегодняшний момент вся функциональность KVM доступна в QEMU, но это не принципиално так как бОльшая часть пользователей виртуализации на Linux не использует напрямую KVM/QEMU, а обращается к ним как минимум через один уровень абстракции, но об этом позже.
</blockquote>

Сегодня VMWare ESXi и Linux KVM/QEMU это два основных гипервизора, которые доминируют на рынке. Они же являются представителями двух разных типов гипервизоров:
<ul>
    <li>Type 1 - гипервизор запускается непосредственно на железе (bare-metal). Таковым является VMWare ESXi.
    <li>Type 2 - гипервизор запускается внутри Host OS (операционной системы). Таковым является Linux KVM.
</ul>

Обсуждение что лучше, а что хуже выходит за рамки данной статьи.

<img src="https://fs.linkmeup.ru/images/adsm/1/1/hypervisors_types.gif" width="400">

Производители железа (в частности CPU) также должны были сделать свою часть работы, дабы обеспечить приемлемую производительность своих процессоров, когда они используются гипервизором для обеспечения ресурсами виртуальной машины.

Пожалуй, наиболее важной и самой широко используемой является технология Intel VT (Virtualization Technology) - набор расширений разработанных Intel для своих x86 процессоров, которые используются для эффективной работы гипервизора (а в некоторых случаях необходимы, так, например, KVM не заработает без включенного VT-x).
Наиболее известны два из этих расширений - VT-x и VT-d. Первое важно для улучшения производительности CPU при виртуализации, так как обеспечивает аппаратную поддержку некторых ее функций, второе для подключения физических устройств напрямую в виртуальную машину (для SRIOV, например, VT-d должен быть включен - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_host_configuration_and_guest_installation_guide/sect-virtualization_host_configuration_and_guest_installation_guide-sr_iov-how_sr_iov_libvirt_works ).

Следующей важной концепцией является отличие полной виртуализации (full virtualization) от пара-виртуализации (para-virtualization).
Полная виртуализация это хорошо, это позволяет запускать какую угодно операционную систему на каком угодно процессоре, однако, это крайне неэффективно и абсолютно не подходит для высоконагруженных систем.
Пара-виртуализация, если коротко, это когда Guest OS понимает что она запущена в виртуальной среде и кооперируется с гипервизором для достижения большей эффективности. То есть появляется guest-hypervisor интерфейс.
Подавляющее большинство используемых операционных систем сегодня имеют поддержку пара-виртуализации - в Linux kernel это появилось начиная с ядра версии 2.6.2.

Для работы виртуальной машины нужны не только виртуальный процессор (vCPU) и виртуальная память (RAM), требуется также эмуляции PCI устройств. То есть по сути, требуется набор драйверов для управления виртуальными сетевыми интерфейсами, дисками и тд.
В гипервизоре Linux KVM данная задача была решена посредством внедрения virtio - фреймворка для разработки и использования виртуализированных устройств ввода/вывода.
Virtio представляет из себя дополнительный уровень абстракции, который позволяет эмулировать различные I/O устройства в пара-виртуализированном гипервизоре предоставляя в сторону виртуальной машины единый и стандартизированный интерфейс. Это позволяет переиспользовать код virtio драйвера для различных по своей сути устройств. Virtio состоит из:
<ul>
    <li> Front-end driver - то что находится в виртуальной машине
    <li> Back-end driver - то что находится в гипервизоре
    <li> Transport driver - то что связывает backend и frontend
</ul>
Эта модульность позволяет изменять технологии, применяемые в гипервизоре, не затрагивая драйверы в виртуальной машине (этот момент очень важен для технологий сетевой акселерации и Cloud решений в целом, но об этом позже).
Однако, связь guest-hypervisor существует, так как речь идет о пара-виртуализации.
<blockquote>
Если вы хоть раз писали вопрос в RFP или отвечали на вопрос в RFP "Поддерживается ли в вашем продукте virtio?" Это как раз было о поддержке front-end virtio драйвера.
</blockquote>

<a name="RESOURCES"></a>
<h1>Типы виртуальных ресурсов - compute, network, storage</h1>
Из чего же состоит виртуальная машина?
Выделяют три основных вида виртуальных ресурсов:

<ul>
    <li> compute - процессор и оперативная память
    <li> storage - системный диск виртуальной машины и блочные хранилища
    <li> network - сетевые карты и устройства ввода/вывода
</ul>

<hr>

<h1>Compute</h1>


<h2>CPU</h2>
Теоретически QEMU способен эмулировать любой тип процессора и соотвествующие ему флаги и функциональность, но на практике никто не выбирает вручную какие именно CPU флаги QEMU должен передавать в Guest OS, а пользуются двумя основными опциями - либо <b>host passthrough</b>, либо <b>named model</b>. То есть либо передавать в Guest OS тот же тип и флаги процессора, что и в Host OS, либо выбрать из длинного списка подходящую модель (сейчас этот список начинается с Conroe 2006 года и заканчивается Skylake 2016 года. Есть из чего выбрать).

По умолчанию QEMU будет эмулировать процессор, который будет распознан Guest OS как QEMU Virtual CPU. Это не самый оптимальный тип процессора особенно, если приложение, работающее в виртуальной машине, использует CPU флаги для своей работы. Подробнее о разных моделях CPU в QEMU - https://wiki.qemu.org/Features/CPUModels
QEMU/KVM также позволяет контролировать топологию процессора, количество тредов, размер кэша, привязывать vCPU к физическому ядру и много чего еще. Нужно ли это для виртуальной машины или нет, зависит от типа приложения работающего в Guest OS. Например, известный факт, что для приложений, выполняющих обработку пакетов с высоким PPS, важно делать <b>CPU pinning</b>, то есть не позволять передавать физический процессор другим виртуальным машинам.

<h2>Memory</h2>
Далее на очереди оперативная память - RAM. С точки зрения Host OS запущенная с помощью QEMU/KVM виртуальная машина ничем не отличается от любого другого процесса, работающего в user-space операционной системы. Соотвественно и процесс выделения памяти виртуальной машине выполняется теми же вызовами в kernel Host OS, как если бы вы запустили Chrome браузер.

С точки зрения виртуальной машины память ей уже выделена на момент ее запуска, однако в реальности это не так, и kernel Host OS выделяет процессу QEMU/KVM новые участки памяти по мере того как приложение в Guest OS запрашивает дополнительную память (хотя тут тоже может быть исключение, если прямо указать QEMU/KVM выделить всю память вирутальной машине непосредственно при запуске). Память выделяется не байт за байтом, а определенным размером - <b>page</b>. Размер page конфигурируем и теоретически может быть любым, но на практике используется размер 4kB (по умолчанию), 2MB и 1GB. Два последних размера называются <b>HugePages</b> и часто используются для выделения памяти для memory intensive виртуальных машин. Причина использования HugePages в процессе поиска соотвествия между виртуальным адресом page и физической памятью в <b>Translation Lookaside Buffer</b> (<b>TLB</b>), который в свою очередь ограничен и хранит информацию только о последних использованных pages. Если информации о нужной page в TLB нет, происходит процесс, называемый <b>TLB miss</b>, и требуется задействовать процессор Host OS для поиска ячейки физической памяти, соотвествующей нужной page. Данный процесс неэффективен и медлителен, поэтому и используется меньшее количество pages бОльшего размера.
QEMU/KVM также позволяет эмулировать различные <b><a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access" target="_blank">NUMA</a></b> топологии для Guest OS, брать память для виртуальной машины только из определенной NUMA Host OS и так далее. Наиболее распространенная практика - брать память память для виртуальной машины из NUMA локальной по отношению к процессорам, выделенным для виртуальной машины. Причина - желание избежать лишней нагрузки на <b>QPI</b> шину, соединяющую CPU sockets физического сервера (само собой, это логично есть в вашем сервере 2 и более sockets).

<hr>

<h1>Storage</h1>
Виртуальная машина нуждается в <b>persistent storage</b>, однако, как это сделать, если виртуальная машина "живет" в оперативной памяти Host OS? (кстати, именно поэтому невозможно запустить виртуальную машину с оперативной памятью меньше чем размер ее образа). Если вкратце, то любое обращение Guest OS к виртуальному IDE контроллеру перехватывается QEMU/KVM и трансформируется в запись на физический диск Host OS. Этот метод неэффективен, и поэтому здесь так же как и для сетевых устройств используется VirtIO драйвер вместо полной эмуляции IDE или iSCSI устройства. Подробнее об этом можно почитать здесь - https://www.qemu.org/2018/02/09/understanding-qemu-devices/. Таким образом, виртуальная машина обращается к своему виртуальному диску через virtio драйвер, а далее QEMU/KVM делает так чтобы переданная информация записалась на физический диск. Важно понимать, что в Host OS дисковый backend может быть реализован в виде CEPH, NFS или iSCSI полки.

Наиболее простым способом эмулировать persistent storage является использование файла в какой-либо директории Host OS как дискового пространства виртуальной машины. QEMU/KVM поддерживает множество различных форматов такого рода файлов - raw, vdi, vmdk и прочие. Однако наибольшее распространение получил формат <b>qcow2</b> (QEMU copy-on-write version 2). В общем случае, qcow2 представляет собой определенным образом структурированный файл без какой-либо операционной системы. Большое количество виртуальных машин распространяется именно в виде qcow2-образов (images) и являются копией системного диска виртуальной машины, упакованной в qcow2 формат. Это имеет ряд преимуществ - qcow2 кодирование занимает гораздо меньше места, чем raw копия диска байт в байт, QEMU/KVM умеет изменять размер qcow2 файла (resizing), а значит имеется возможность изменить размер системного диска виртуальной машины, также поддерживается AES шифрование qcow2 (это имеет смысл, так как образ виртуальной машины может содержать интеллектуальную собственность).

Далее, когда происходит запуск виртуальной машины, QEMU/KVM использует qcow2 файл как системный диск (процесс загрузки виртуальной машины я опускаю здесь, хотя это тоже является интересной задачей), а виртуальная машина имеет возможность считать/записать данные в qcow2 файл через virtio драйвер. Таким образом и работает процесс снятия образов виртуальных машин, поскольку в любой момент времени qcow2 файл содержит полную копию системного диска виртульной машины, и образ может быть использован для резервного копирования, переноса на другой хост и прочее.

В общем случае этот qcow2 файл будет определяться в Guest OS как <i>/dev/vda</i> устройство, и Guest OS произведет партирование и установку файловой системы. Аналогично, следующие qcow2 файлы, подключенные QEMU/KVM как /dev/vdX устройства, могут быть использованы как <b>block storage</b> в виртуальной машине для хранения информации (именно так и работает компонент Openstack Cinder).
<hr>

<h1>Network</h1>
Последним в нашем списке виртуальных ресурсов идут сетевые карты и устройства ввода/вывода. Виртуальная машина, как и физический хост, нуждается в <b>PCI/PCIe шине</b> для подключения устройств ввода/вывода. QEMU/KVM способен эмулировать разные типы PCI - q35 или i440fx (первый - PCIe, второй - legacy PCI ), а также различные PCI топологии, например, создавать отдельные PCI шины (PCI expander bus) в различных Guest OS NUMA.

После создания PCI/PCIe шины необходимо подключить к нему устройство ввода/вывода. В общем случае это может быть что угодно - от мышки до физического GPU. И, конечно же, сетевая карта, как полностью виртуализированная (e1000, например), так и пара-виртуализированная (virtio, например) или физическая NIC. Последняя опция используется для data-plane виртуальных машин, где требуется получить line-rate скорости передачи пакетов. Здесь существует два основных подхода - <b>PCI passthrough</b> и <b>SR-IOV</b>. Основное отличие между ними - для PCI-PT используется драйвер внутри Guest OS, а для SRIOV  используется драйвер Host OS (для создания <b>VF - Virtual Functions</b>b>) и драйвер Guest OS для управления SR-IOV VF.

Таким образом, мы рассмотрели основные виды виртуальных ресурсов и следующим шагом необходимо понять как виртуальная машина общается с внешним миром через сеть.
<hr>

<a name="SWITCHING"></a>
<h1>Виртуальная коммутация</h1>

Если есть виртуальная машина, а в ней есть виртуальный интерфейс, то очевидно возникает задача передачи пакета из одной VM в другую. В Linux-based гипервизорах (KVM, например) эта задача может решаться с помощью Linux bridge, однако, большое распространение получил проект Open vSwitch (OVS) - https://www.openvswitch.org.
Есть несколько основных функциональностей, которые позволили OVS широко распространиться и стать de-facto основным software, которое используется во многих Cloud (например, Openstack) и виртуализированных решениях.
<ul>
    <li> Передача сетевого состояния - при миграции VM между гипервизорами возникает задача передачи ACL, QoSs, L2/L3 forwardding таблиц и прочего. И OVS умеет это
    <li> Реализация механизма передачи пакетов (datapath) как в kernel, так и в user-space
    <li> CUPS архитектура - позволяет перенести функционал обработки пакетов на специализированный chipset (Broadcom and Marvell chipset, например, могут такое) , управляя им control-plane OVS.
    <li> Поддержка метолов удаленного управления траффиком - протокол OpenFlow (привет, SDN)
</ul>

Архитектура OVS на первый взгляд выглядит довольно страшно, но это только на первый взгляд.
<img src="https://fs.linkmeup.ru/images/adsm/1/1/ovs_architecture_01.png" width="400">

Для работы с OVS нужно понимать следующее:
<ul>
<li> Datapath - тут обрабатываются пакеты. Аналогия - switch-fabric железного коммутатора. Если OVS работает в kernel, то выполнен в виде модуля ядра. Если OVS работает в user-space, то это процесс в user-space Linux.
<li> vswitchd и ovsdb - daemons в user-space, то что реализует непосредственно сам функционал коммутатора, хранит конфигурацию, устанавливает flow в datapath
 <li> Набор инструментов для настройки и траблшутинга OVS - ovs-vsctl, ovs-dpctl, ovs-ofctl, ovs-appctl. Все то что нужно чтобы прописать в ovsdb конфигурацию портов, прописать какой flow куда должен коммутироваться, собрать статистику и прочее. Добрые люди написали статью по этому поводу http://therandomsecurityguy.com/openvswitch-cheat-sheet/
</ul>

Каким же образом сетевое устройство виртуальной машины оказывается в OVS?
