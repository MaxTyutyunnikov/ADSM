Предыдущая статья рассматривала архитектуру виртуализированной сети, underlay-overlay, путь пакета между VM и прочее. В данной статье мы затронем (или попытаемся затронуть) вопросы а как собственно происходит виртаулизация сетевых функций, как реализован backend основных продуктов обеспечивающих запуск и управление VM, а также как работает виртуальный свитчинг (OVS и Linux bridge).

Тема виртуализации широка и глубока, объяснить все детали работы гипервизора невозможно (да и не нужно). Мы ограничимся минимальным набором знаний необходимым для понимания работы любого виртуализированного решения, не обязательно Telco.

<h1>Содержание</h1>
<ul>
    <li><b><a href="#INTRODUCTION">Введение и история краткая виртуализации</a></b></li>
    <li><b><a href="#SWITCHING">Виртуальная коммутация</a></b></li>
    <li><b><a href="#RESOURCES">Типы виртуальных ресурсов - compute, network, storage</a></b></li>
    <li><b><a href="#INSTRUMENTS">Инструменты и технология виртуализации - libvirt, qemu-kvm и прочее</a></b>
    <li><b><a href="#PACKETPATH">Путь пакета в виртуальной сети</a></b>
    <li><b><a href="#SDN">Использование виртуальной коммутации в Cloud платформах</a></b>
    <li><b><a href="#PROBLEMS">Известные проблемы и сложности виртуализации </a></b>
</ul>
<hr>

<li><a name="INTRODUCTION"></a>
<h1>Введение и краткая история виртуализации</h1>

История современных технологий виртуализации берет свое начало в 1999 году, когда молодая компания VMWare выпустила продукт под названием VMWare Workstation. Это был продукт обеспечивающий виртуализацию desktop/client приложений. Виртуализация серверной части пришла несколько позднее в виде продукта ESX Server который в дальнейшем эволюционировал в ESXi (i означает integrated) - это тот самый продукт, который используется повсеместно как в IT так и в Telco как гипервизор серверных приложений.

На стороне Opensource два основных проекта принесли виртуализацию в Linux:
<ul>
    <li>KVM (Kernel-based Virtual Machnine) - модуль ядра Linux который позволяет kernel работать как гипервизор (создает необходимую инфраструктуру для запуска и управления VM). Был добавлен в версии ядра 2.6.20 в 2007 году</li>
    <li>QEMU (Quick Emulator) - непосредственно эмулирует железо для виртуальной машины (CPU, Disk, RAM, что угодно включая USB порт) и используется совместно с KVM для достижения почти "native" производительности</li>
</ul>

<blockquote>
На самом деле на сегодняшний момент вся функциональность KVM доступна в QEMU, но это не принципиално так как бОльшая часть пользователей виртуализации на Linux не использует напрямую KVM/QEMU, а обращается к ним как минимум через один уровень абстракции, но об этом позже.
</blockquote>

Сегодня VMWare ESXi и Linux KVM/QEMU это два основных гипервизора, которые доминируют на рынке. Они же являются представителями двух разных типов гипервизоров:
<ul>
    <li>Type 1 - гипервизор запускается непосредственно на железе (bare-metal). Таковым является VMWare ESXi.
    <li>Type 2 - гипервизор запускается внутри Host OS (операционной системы). Таковым является Linux KVM.
</ul>

Обсуждение что лучше, а что хуже выходит за рамки данной статьи.

<img src="https://fs.linkmeup.ru/images/adsm/1/1/hypervisors_types.gif" width="400">

Производители железа (в частности CPU) также должны были сделать свою часть работы, дабы обеспечить приемлимую производительность своих процессоров когда они используются гипервизором для обеспечения ресурсами виртуальной машины.

Пожалуй, наиболее важной и самой широко используемой является технология Intel VT (Virtualization Technology) - набор расширений разработанных Intel для своих x86 процессоров, которые используются для эффективной работы гипервизора (а в некоторых случаях необходимы, так например KVM не заработает без включенного VT-x).
Наиболее известны два из этих расширений - VT-x и VT-d. Первое важно для улучшения производительности CPU при виртуализации, так как обеспечивает аппаратную поддержку некторых ее функций, второе для подключения физических устройств напрямую в виртуальную машину (для SRIOV, например, VT-d должен быть включен - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_host_configuration_and_guest_installation_guide/sect-virtualization_host_configuration_and_guest_installation_guide-sr_iov-how_sr_iov_libvirt_works ).

Следующей важной концепцией является отличие полной виртуализации (full virtualization) от пара-виртуализации (para-virtualization).
Полная виртуализация это хорошо, это позволяет запускать какую угодно операционную систему на каком угодно процессоре, однако, это крайне неэффективно и абсолютно не подходит для высоконагруженных систем.
Пара-виртуализация, если коротко, это когда Guest OS понимает что она запущена в виртуальной среде и кооперируется с гипервизором для достижения большей эффективности. То есть появляется guest-hypervisor интерфейс.
Подавляющее большинство используемых операционных систем сегодня имеют поддержку пара-виртуализации - в Linux kernel это появилось начиная с ядра версии 2.6.2.

Для работы виртуальной машины нужны не только виртуальный процессор (vCPU) и виртуальная память (RAM), требуется также эмуляции PCI устройств. То есть по сути, требуется набор драйверов для управления виртуальными сетевыми интерфейсами, дисками и тд.
В гипервизоре Linux KVM данная задача была решена посредством внедрения virtio - фреймворка для разработки и использования виртуализированных устройств ввода/вывода.
Virtio представляет из себя дополнительный уровень абстракции, который позволяет эмулировать различные I/O устройства в пара-виртуализированном гипервизоре предоставляя в сторону виртуальной машины единый и стандартизированный интерфейс. Это позволяет переиспользовать код virtio драйвера для различных по своей сути устройств. Virtio состоит из:
<ul>
    <li> Front-end driver - то что находится в виртуальной машине
    <li> Back-end driver - то что находится в гипервизоре
    <li> Transport driver - то что связывает backend и frontend
</ul>
Эта модульность позволяет изменять технологии применяемые в гипервизоре не затрагивая драйверы в виртуальной машине (этот момент очень важен для технологий сетевой акселерации и Cloud решений в целом, но об этом позже).
Однако, связь guest-hypervisor существует, так как речь идет о пара-виртуализации.
<blockquote>
Если вы хоть раз писали вопрос в RFP или отвечали на вопрос в RFP "Поддерживается ли в вашем продукте virtio?" Это как раз было о поддержке front-end virtio драйвера.
</blockquote>

<li><a name="SWITCHING"></a>
<h1>Виртуальная коммутация</h1>

Если есть виртуальная машина, а в ней есть виртуальный интерфейс, то очевидно возникает задача передачи пакета из одной VM в другую. В Linux-based гипервизорах (KVM, например) эта задача может решаться с помощью Linux bridge, однако, большое распространение получил проект Open vSwitch (OVS) - https://www.openvswitch.org.
Есть несколько основных функциональностей, которые позволили OVS широко распространиться и стать de-facto основным software, которое используется во многих Cloud (например, Openstack) и виртуализированных решениях.
<ul>
    <li> Передача сетевого состояния - при миграции VM между гипервизорами возникает задача передачи ACL, QoSs, L2/L3 forwardding таблиц и прочего. И OVS умеет это
    <li> Реализация механизма передачи пакетов (datapath) как в kernel, так и в user-space
    <li> CUPS архитектура - позволяет перенести функционал обработки пакетов на специализированный chipset (Broadcom and Marvell chipset например могут такое) , управляя им control-plane OVS.
    <li> Поддержка метолов удаленного управления траффиком - протокол OpenFlow (привет, SDN)
</ul>

Архитектура OVS на первый взгляд выглядит довольно страшно, но это только на первый взгляд.
<img src="https://fs.linkmeup.ru/images/adsm/1/1/ovs_architecture_01.png" width="400">

Для работы с OVS нужно понимать следующее:
<ul>
<li> Datapath - тут обрабатываются пакеты. Аналогия - switch-fabric железного коммутатора. Если OVS работает в kernel, то выполнен в виде модуля ядра. Если OVS работает в user-space, то это процесс в user-space Linux.
<li> vswitchd и ovsdb - daemons в user-space, то что реализует непосредственно сам функционал коммутатора, хранит конфигурацию, устанавливает flow в datapath
 <li> Набор инструментов для настройки и траблшутинга OVS - ovs-vsctl, ovs-dpctl, ovs-ofctl, ovs-appctl. Все то что нужно чтобы прописать в ovsdb конфигурацию портов, прописать какой flow куда должен коммутироваться, собрать статистику и прочее. Добрые люди написали статью по этому поводу http://therandomsecurityguy.com/openvswitch-cheat-sheet/
</ul>

Каким же образом сетевое устройство виртуальной машины оказывается в OVS?
