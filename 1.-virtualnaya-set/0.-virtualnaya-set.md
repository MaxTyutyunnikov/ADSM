Терминология


Вся серия будет описывать ДЦ, состоящий из рядов однотипных стоек, в которых установлено одинаковое оборудование. 
На этом оборудовании запускаются виртуальные машины/контейнеры, реализующие сервисы.

В статье сервером мы будем называть программу, которая реализует серверную сторону клиент-серверной коммуникации.
Физические машины в стойках называть серверами не будем.

Физическая машина - x86-компьютер, установленный в стойке. Будем называть её "машина".
Виртуальная машина - операционная система, запущенная на физической машине. Будем называть её "ВМ".
Гипервизор - приложение, запущенное на физической машине, позволяющее управлять виртуальными машинами.

Поскольку гипервизор на каждой машине только один, иногда я буду использовать "гипервизор" как синоним "физической машине".
ToR - Top of the Rack - коммутатор, установленный в стойке, к котором подключены все физические машины.



Сеть:
Leaf/Spine.

Картинка со всей топологией!!!!

Сеть делится на Underlay и Overlay.

Underlay - это весьма статическая вещь, опирающаяся на аппаратные коммутаторы и базовые протоколы и технологии.
!!! картинке 

Underlay сравнительно редко меняется, потому что его задача - базовая IP-связность между физическими машинами. Underlay ничего не знает о запущенных поверх него сервисах, клиентах, тенантах - ему нужно только доставить пакет от одной машины до другой.
Underlay может быть например таким: 
IPv4+OSPF
Или 
IPv6+ISIS+BGP+L3VPN
Или
L2+TRILL
Или
L2+STP

Настраивается Underlay'ная сеть классическим образом: CLI/GUI/NETCONF.
Вручную, скриптами, проприетарными утилитами.

Overlay - виртуальная сеть туннелей, натянутая поверх Underlay.
Так ВМ одного клиента (одного сервиса) могут общаться друг с другом через Overlay, даже не подозревая какой на самом деле путь проходит пакет. 
Overlay может быть например таким:
GRE-туннель
VxLAN
EVPN
L3VPN
Overlay обычно настраивается и поддерживается через центральный контроллер, конфигурация и data plane конечные виртуальные устройства получают уже с него автоматически.
Да, это SDN в чистом виде.


Проще всего рассмотреть на примерах:

    1. Коммуникация внутри одной физической машины.

ВМ1 хочет отправить пакет на ВМ2.
У ВМ1 есть маршрут по умолчанию в его интерфейс eth0. Пакет отправляется туда.
Этот интерфейс Eth0 на самом деле виртуально соединён с vSwitch.
vSwitch анализирует на какой интерфейс пришёл пакет, то есть к какому клиенту (VRF) он относится, сверяет адрес получателя с таблицей маршрутизации этого клиента.
Обнаружив, что получателя на этой же машине за другим портом, vSwitch просто отправляет пакет в него без каких-либо дополнительных заголовков.

    2. Коммуникация между ВМ, расположенными на разных физических машинах.
Начало точно такое же. Только теперь vSwitchвидит, что адресат находится на другой машине и доступен через туннель Tunnel0.
У Tunnel0 источник ip1, получатель: ip2.
В таблице маршрутизации vSwitch есть маршрут по умолчанию через адрес gw1.
vSwitch добавляет новые заголовки к исходному пакету и отправляет его в ToR.
!!!! Заголовки

ToR как участник Underlay сети знает, как добраться до ip2 и отправляет пакет по маршруту.
При этом знать, что находится под внешним заголовком IP ему не нужно. То есть фактически под IP может быть бутерброд из IPv6 over MPLS over Ethernet over MPLS over GRE over over over.
Соответственно на принимающей стороне vSwitch по некоторому идентификатору в туннельном заголовке понимает, какому клиенту этот пакет надо передать, раздевает его и отправляет в первоначальном виде получателю.

Overlay может меняться хоть каждую минуту. Центральный контроллер берёт на себя все сложности с поддержанием конфигурации и контролем таблиц коммутации на виртуальных устройствах.

При этом не меняется никоим образом конфигурация Underlay-сети, которую кстати, автоматизировать на порядок сложнее, а сломать неловким движением проще.

Выход во внешний мир
Где-то симуляция должна закончиться и из виртуального мира выходить в реальный.
Практикуют два подхода:
    1) В качестве ВМ запускает какой-либо appliance, реализующий функции маршрутизатора (да-да, вслед за SDN мы и с VNF столкнулись). Назовём его виртуальный шлюз.
    Одной своей ногой он смотрит в виртуальную сеть и может взаимодействовать с другими ВМ. При этом она может терминировать на себе сети всех клиентов и, соответственно, осуществлять и маршрутизацию между ними при необходимости.
    Этот шлюз так же взаимодействует с контроллером, от которого другие ВМ получают информацию о том, где запущен этот шлюз, и наоборот.
    Другой ногой шлюз смотрит уже в реальную сеть. У него есть сессия с RR, откуда он получает информация о том как добраться до сетей внешнего мира, у него есть маршруты до бордеров.
    
    
То есть процесс выглядит так: 
ВМ1 отправляет пакет в eth0 с адресатом во внешнем мире.
vSwitch знает от контроллера, что маршрут по умолчанию лежит через шлюз  VGW1.
Упаковывает первоначальный пакет в туннель и отправляет на ToR.
Underlay доставляет пакет до шлюза.
Шлюз снимает туннелирующие заголовки, видит адрес, консультируется со своей таблицей маршрутизации и отправляет первоначальный пакет в сторону бордера. 

В качестве шлюза может быть аппаратный маршрутизатор или виртуальный апплайнс, реализующий функции маршрутизатора. Главное, чтобы он мог взаимодействовать с контроллером. Благо мы имеем такой классный протокол, как BGP.



В этой главе я посвятил очень мало внимания Underlay-сети, остановившись подробнее на Overlay. Это потому, что далее в серии я больше не буду касаться последней и сосредоточусь на первой.